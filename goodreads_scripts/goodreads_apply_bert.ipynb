{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515eace7-1e4b-42cf-9562-521f9bd2633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BERT Emotion Analysis Configuration initialized\n",
      "ü§ñ Device: cuda\n",
      "üì¶ Batch size: 16\n",
      "üìè Max length: 512\n",
      "‚úÖ Artemis emotion modules imported successfully\n",
      "üìä Emotion classes: ['amusement', 'awe', 'contentment', 'excitement', 'anger', 'disgust', 'fear', 'sadness', 'something else']\n",
      "üìä Number of labels: 9\n",
      "üìã BERT EMOTION ANALYSIS CONFIGURATION\n",
      "--------------------------------------------------\n",
      "üìÅ Artemis path: data/artemis/artemis\n",
      "ü§ñ BERT model directory: data/artemis/artemis/predictions/bert_based/best_model\n",
      "üìä Data file: preprocessed_books_2025_04_20.parquet\n",
      "üíæ Results directory: goodreads_bert_emotion_results\n",
      "üñ•Ô∏è Device: cuda\n",
      "üì¶ Batch size: 16\n",
      "üìè Max sequence length: 512\n",
      "üéØ Expected emotion classes: 9\n",
      "--------------------------------------------------\n",
      "‚úÖ Model directory found with files: ['model.safetensors', 'config.json', 'tokenizer_config.json', 'vocab.txt', 'special_tokens_map.json', 'tokenizer.json']\n",
      "Ready to run! Execute: results = analyze_goodreads_bert_emotions()\n",
      "ü§ñ BERT-based Goodreads Book Description Emotion Analysis\n",
      "üìö Processing English books with descriptions only\n",
      "üîÑ Using fine-tuned BERT model from ArtEmis training\n",
      "------------------------------------------------------------\n",
      "ü§ñ Loading fine-tuned BERT model from data/artemis/artemis/predictions/bert_based/best_model\n",
      "üìÅ Files in model directory: ['model.safetensors', 'config.json', 'tokenizer_config.json', 'vocab.txt', 'special_tokens_map.json', 'tokenizer.json']\n",
      "‚úÖ BERT model loaded successfully\n",
      "üìä Model expects 9 emotion classes\n",
      "üéØ Expected emotions: 9\n",
      "üöÄ Starting BERT-based Goodreads emotion analysis...\n",
      "ü§ñ Using model from: data/artemis/artemis/predictions/bert_based/best_model\n",
      "üìÇ Loading Goodreads data...\n",
      "üìä Loaded 931,229 total books\n",
      "üîç Filtering for English books from 931,229 total books\n",
      "‚úÖ Filtered to 687,029 English books\n",
      "üìâ Dropped 244,200 non-English books (26.2%)\n",
      "üîç Filtering books with descriptions from 687,029 books\n",
      "üìù Using description field: 'description'\n",
      "‚úÖ Found 686,946 books with meaningful descriptions\n",
      "üìâ Dropped 83 books without descriptions (0.0%)\n",
      "üìÇ Loaded BERT checkpoint from 2025-06-04T15:21:20.619587\n",
      "üìä Previous progress: 380,000 results, 0 failed\n",
      "üîÑ Resuming from checkpoint: 380,000 books already processed\n",
      "üìã Processing 306,946 remaining English books with descriptions\n",
      "üîÑ Processing 62 batches of up to 5,000 books each\n",
      "üéØ Using BERT batch size of 16 for predictions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7711188518c46f0961cdb35b45af34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing BERT batches:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Progress: 56.0% (385,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 12827.0 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 385,000 results, 0 failed\n",
      "üìä Progress: 56.8% (390,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 5830.6 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 390,000 results, 0 failed\n",
      "üìä Progress: 57.5% (395,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 3873.0 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 395,000 results, 0 failed\n",
      "üìä Progress: 58.2% (400,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 2900.9 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 400,000 results, 0 failed\n",
      "üìä Progress: 59.0% (405,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 2298.5 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 405,000 results, 0 failed\n",
      "üìä Progress: 59.7% (410,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 1896.5 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 410,000 results, 0 failed\n",
      "üìä Progress: 60.4% (415,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 1628.1 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 415,000 results, 0 failed\n",
      "üìä Progress: 61.1% (420,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 1436.0 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 420,000 results, 0 failed\n",
      "üìä Progress: 61.9% (425,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 1283.8 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 425,000 results, 0 failed\n",
      "üìä Progress: 62.6% (430,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 1159.6 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 430,000 results, 0 failed\n",
      "üìä Progress: 63.3% (435,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 1057.1 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 435,000 results, 0 failed\n",
      "üìä Progress: 64.1% (440,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 976.4 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 440,000 results, 0 failed\n",
      "üìä Progress: 64.8% (445,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 908.9 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 445,000 results, 0 failed\n",
      "üìä Progress: 65.5% (450,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 848.5 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 450,000 results, 0 failed\n",
      "üìä Progress: 66.2% (455,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 795.9 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 455,000 results, 0 failed\n",
      "üìä Progress: 67.0% (460,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 749.4 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 460,000 results, 0 failed\n",
      "üìä Progress: 67.7% (465,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 710.3 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 465,000 results, 0 failed\n",
      "üìä Progress: 68.4% (470,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 676.7 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 470,000 results, 0 failed\n",
      "üìä Progress: 69.1% (475,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 642.6 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 475,000 results, 0 failed\n",
      "üìä Progress: 69.9% (480,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 614.6 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 480,000 results, 0 failed\n",
      "üìä Progress: 70.6% (485,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 587.2 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 485,000 results, 0 failed\n",
      "üìä Progress: 71.3% (490,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 563.8 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 490,000 results, 0 failed\n",
      "üìä Progress: 72.1% (495,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 542.2 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 495,000 results, 0 failed\n",
      "üìä Progress: 72.8% (500,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 522.8 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 500,000 results, 0 failed\n",
      "üìä Progress: 73.5% (505,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 504.9 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 505,000 results, 0 failed\n",
      "üìä Progress: 74.2% (510,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 487.5 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 510,000 results, 0 failed\n",
      "üìä Progress: 75.0% (515,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 472.2 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 515,000 results, 0 failed\n",
      "üìä Progress: 75.7% (520,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 458.3 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 520,000 results, 0 failed\n",
      "üìä Progress: 76.4% (525,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 444.5 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 525,000 results, 0 failed\n",
      "üìä Progress: 77.2% (530,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 432.3 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 530,000 results, 0 failed\n",
      "üìä Progress: 77.9% (535,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 421.0 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 535,000 results, 0 failed\n",
      "üìä Progress: 78.6% (540,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 410.0 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 540,000 results, 0 failed\n",
      "üìä Progress: 79.3% (545,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 399.6 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 545,000 results, 0 failed\n",
      "üìä Progress: 80.1% (550,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 390.4 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 550,000 results, 0 failed\n",
      "üìä Progress: 80.8% (555,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 381.3 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 555,000 results, 0 failed\n",
      "üìä Progress: 81.5% (560,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 372.5 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 560,000 results, 0 failed\n",
      "üìä Progress: 82.2% (565,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 364.2 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 565,000 results, 0 failed\n",
      "üìä Progress: 83.0% (570,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 356.6 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 570,000 results, 0 failed\n",
      "üìä Progress: 83.7% (575,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 349.6 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 575,000 results, 0 failed\n",
      "üìä Progress: 84.4% (580,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 343.0 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 580,000 results, 0 failed\n",
      "üìä Progress: 85.2% (585,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 336.6 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 585,000 results, 0 failed\n",
      "üìä Progress: 85.9% (590,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 330.2 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 590,000 results, 0 failed\n",
      "üìä Progress: 86.6% (595,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 324.3 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 595,000 results, 0 failed\n",
      "üìä Progress: 87.3% (600,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 318.5 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 600,000 results, 0 failed\n",
      "üìä Progress: 88.1% (605,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 313.2 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 605,000 results, 0 failed\n",
      "üìä Progress: 88.8% (610,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 308.1 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 610,000 results, 0 failed\n",
      "üìä Progress: 89.5% (615,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 303.1 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 615,000 results, 0 failed\n",
      "üìä Progress: 90.3% (620,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 298.6 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 620,000 results, 0 failed\n",
      "üìä Progress: 91.0% (625,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 294.0 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 625,000 results, 0 failed\n",
      "üìä Progress: 91.7% (630,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 289.5 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 630,000 results, 0 failed\n",
      "üìä Progress: 92.4% (635,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 285.5 books/sec | ‚è±Ô∏è ETA: 0.1h\n",
      "üíæ BERT Checkpoint saved: 635,000 results, 0 failed\n",
      "üìä Progress: 93.2% (640,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 281.6 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 640,000 results, 0 failed\n",
      "üìä Progress: 93.9% (645,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 277.6 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 645,000 results, 0 failed\n",
      "üìä Progress: 94.6% (650,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 273.7 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 650,000 results, 0 failed\n",
      "üìä Progress: 95.3% (655,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 270.2 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 655,000 results, 0 failed\n",
      "üìä Progress: 96.1% (660,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 266.8 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 660,000 results, 0 failed\n",
      "üìä Progress: 96.8% (665,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 263.3 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 665,000 results, 0 failed\n",
      "üìä Progress: 97.5% (670,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 259.9 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 670,000 results, 0 failed\n",
      "üìä Progress: 98.3% (675,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 256.7 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 675,000 results, 0 failed\n",
      "üìä Progress: 99.0% (680,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 253.8 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 680,000 results, 0 failed\n",
      "üìä Progress: 99.7% (685,000/686,946) | ‚úÖ Success: 100.0% | üöÄ Speed: 250.8 books/sec | ‚è±Ô∏è ETA: 0.0h\n",
      "üíæ BERT Checkpoint saved: 685,000 results, 0 failed\n",
      "üíæ BERT Checkpoint saved: 686,946 results, 0 failed\n",
      "\n",
      "============================================================\n",
      "üéâ BERT EMOTION ANALYSIS COMPLETED!\n",
      "============================================================\n",
      "üìä Total processed: 686,946\n",
      "‚úÖ Successful: 686,946 (100.00%)\n",
      "‚ùå Failed: 0\n",
      "üíæ Results saved to: goodreads_bert_emotion_results/goodreads_bert_emotion_predictions_20250604_160958.parquet\n",
      "============================================================\n",
      "\n",
      "üìà Emotion Distribution:\n",
      "  sadness: 192,494 (28.0%)\n",
      "  amusement: 110,417 (16.1%)\n",
      "  excitement: 100,331 (14.6%)\n",
      "  fear: 94,141 (13.7%)\n",
      "  awe: 89,756 (13.1%)\n",
      "  anger: 35,642 (5.2%)\n",
      "  contentment: 26,742 (3.9%)\n",
      "  something else: 19,989 (2.9%)\n",
      "  disgust: 17,434 (2.5%)\n",
      "\n",
      "üìä Emotion Categories (Positive/Negative/Else):\n",
      "  1: 339,711 (49.5%)\n",
      "  0: 327,246 (47.6%)\n",
      "  2: 19,989 (2.9%)\n",
      "\n",
      "üéØ Confidence Statistics:\n",
      "  Mean confidence: 0.481\n",
      "  Median confidence: 0.441\n",
      "  High confidence (>0.8): 54,237 (7.9%)\n"
     ]
    }
   ],
   "source": [
    "# Goodreads Book Description Emotion Analysis using Fine-tuned BERT\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "from queue import Queue\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import unicodedata\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Enable nested async loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('goodreads_bert_emotion_analysis.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration matching your BERT training script\n",
    "class BERTEmotionConfig:\n",
    "    # Paths - UPDATE THESE TO MATCH YOUR SETUP\n",
    "    ARTEMIS_PATH = r'data/artemis/artemis'  # Your artemis path\n",
    "    BERT_MODEL_DIR = r'data/artemis/artemis/predictions/bert_based/best_model'  # Your trained BERT model path\n",
    "    DATA_FILE = 'preprocessed_books_2025_04_20.parquet'  # Your Goodreads data\n",
    "    RESULTS_DIR = 'goodreads_bert_emotion_results'\n",
    "    \n",
    "    # BERT configuration (matching your training script exactly)\n",
    "    MAX_LENGTH = 512  # Same as your training\n",
    "    BATCH_SIZE = 16   # Same as your training script\n",
    "    MODEL_NAME = 'google-bert/bert-base-uncased'  # Same base model\n",
    "    \n",
    "    # Processing configuration\n",
    "    PROCESSING_BATCH_SIZE = 5000  # Smaller batches for BERT\n",
    "    CHECKPOINT_FREQUENCY = 2500\n",
    "    \n",
    "    # Progress tracking\n",
    "    PROGRESS_UPDATE_FREQUENCY = 500\n",
    "    DETAILED_LOG_FREQUENCY = 1000\n",
    "    \n",
    "    # English language filtering\n",
    "    ENGLISH_CODES = {\n",
    "        'en', 'eng', 'en-us', 'en-gb', 'en-ca', 'en-au', 'en-nz', 'en-za', \n",
    "        'en-in', 'english', 'en_us', 'en_gb', 'en_ca', 'en_au'\n",
    "    }\n",
    "    \n",
    "    # GPU configuration\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def __init__(self):\n",
    "        os.makedirs(self.RESULTS_DIR, exist_ok=True)\n",
    "        os.makedirs(f\"{self.RESULTS_DIR}/checkpoints\", exist_ok=True)\n",
    "        print(f\"‚úÖ BERT Emotion Analysis Configuration initialized\")\n",
    "        print(f\"ü§ñ Device: {self.DEVICE}\")\n",
    "        print(f\"üì¶ Batch size: {self.BATCH_SIZE}\")\n",
    "        print(f\"üìè Max length: {self.MAX_LENGTH}\")\n",
    "\n",
    "config = BERTEmotionConfig()\n",
    "\n",
    "# Add artemis to path\n",
    "# if config.ARTEMIS_PATH not in sys.path:\n",
    "#     sys.path.append(config.ARTEMIS_PATH)\n",
    "\n",
    "# Import artemis modules (matching your training script)\n",
    "try:\n",
    "    from artemis.emotions import ARTEMIS_EMOTIONS, IDX_TO_EMOTION, positive_negative_else\n",
    "    from artemis.in_out.basics import create_dir\n",
    "    print(\"‚úÖ Artemis emotion modules imported successfully\")\n",
    "    print(f\"üìä Emotion classes: {ARTEMIS_EMOTIONS}\")\n",
    "    print(f\"üìä Number of labels: {len(ARTEMIS_EMOTIONS)}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing artemis modules: {e}\")\n",
    "    print(\"Please check your ARTEMIS_PATH in the config above\")\n",
    "\n",
    "def preprocess_text_artemis_style(text):\n",
    "    \"\"\"\n",
    "    Preprocess text following ArtEmis conventions\n",
    "    This should match the preprocessing used in utterance_spelled\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and normalize unicode (like ArtEmis does)\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Basic cleaning while preserving meaningful content\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove very long sequences that might cause issues\n",
    "    # But keep the text mostly intact since BERT can handle various formats\n",
    "    if len(text) > 10000:  # Very long descriptions\n",
    "        text = text[:10000] + \"...\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "def filter_english_books(books_df):\n",
    "    \"\"\"Filter books to include only English language books\"\"\"\n",
    "    print(f\"üîç Filtering for English books from {len(books_df):,} total books\")\n",
    "    \n",
    "    def is_english(lang_code):\n",
    "        if pd.isna(lang_code) or lang_code == '':\n",
    "            return False\n",
    "        lang_code_clean = str(lang_code).lower().strip()\n",
    "        return lang_code_clean in config.ENGLISH_CODES\n",
    "    english_mask = books_df['language_code'].astype(str).apply(is_english)\n",
    "    english_books = books_df[english_mask].copy()\n",
    "    \n",
    "    dropped_count = len(books_df) - len(english_books)\n",
    "    print(f\"‚úÖ Filtered to {len(english_books):,} English books\")\n",
    "    print(f\"üìâ Dropped {dropped_count:,} non-English books ({dropped_count/len(books_df)*100:.1f}%)\")\n",
    "    \n",
    "    return english_books\n",
    "\n",
    "def filter_books_with_descriptions(books_df):\n",
    "    \"\"\"Filter books that have descriptions\"\"\"\n",
    "    print(f\"üîç Filtering books with descriptions from {len(books_df):,} books\")\n",
    "    \n",
    "    # Check for description fields (common field names in Goodreads data)\n",
    "    description_fields = ['description', 'book_description', 'summary', 'plot', 'desc']\n",
    "    description_field = None\n",
    "    \n",
    "    for field in description_fields:\n",
    "        if field in books_df.columns:\n",
    "            description_field = field\n",
    "            break\n",
    "    \n",
    "    if description_field is None:\n",
    "        print(\"‚ùå No description field found in the dataset\")\n",
    "        print(f\"Available columns: {list(books_df.columns)}\")\n",
    "        # Return empty dataframe if no description field\n",
    "        return books_df.iloc[:0].copy(), None\n",
    "    \n",
    "    print(f\"üìù Using description field: '{description_field}'\")\n",
    "    \n",
    "    # Filter books with non-empty descriptions\n",
    "    has_description = (\n",
    "        books_df[description_field].notna() & \n",
    "        (books_df[description_field].astype(str).str.strip() != '') &\n",
    "        (books_df[description_field].astype(str).str.len() > 10)  # At least 10 characters\n",
    "    )\n",
    "    \n",
    "    books_with_desc = books_df[has_description].copy()\n",
    "    \n",
    "    dropped_count = len(books_df) - len(books_with_desc)\n",
    "    print(f\"‚úÖ Found {len(books_with_desc):,} books with meaningful descriptions\")\n",
    "    print(f\"üìâ Dropped {dropped_count:,} books without descriptions ({dropped_count/len(books_df)*100:.1f}%)\")\n",
    "    \n",
    "    return books_with_desc, description_field\n",
    "\n",
    "class BERTProgressTracker:\n",
    "    \"\"\"Progress tracker for BERT emotion analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, total_books):\n",
    "        self.total_books = total_books\n",
    "        self.processed_books = 0\n",
    "        self.successful_books = 0\n",
    "        self.failed_books = 0\n",
    "        self.start_time = time.time()\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # Performance metrics\n",
    "        self.books_per_second = 0\n",
    "        \n",
    "    def update(self, successful=0, failed=0):\n",
    "        with self.lock:\n",
    "            self.successful_books += successful\n",
    "            self.failed_books += failed\n",
    "            self.processed_books = self.successful_books + self.failed_books\n",
    "            \n",
    "            # Calculate performance\n",
    "            elapsed = time.time() - self.start_time\n",
    "            if elapsed > 0:\n",
    "                self.books_per_second = self.processed_books / elapsed\n",
    "    \n",
    "    def should_log(self, frequency):\n",
    "        return self.processed_books % frequency == 0\n",
    "    \n",
    "    def get_status_message(self):\n",
    "        with self.lock:\n",
    "            progress_pct = (self.processed_books / self.total_books) * 100\n",
    "            success_rate = (self.successful_books / max(1, self.processed_books)) * 100\n",
    "            \n",
    "            remaining = self.total_books - self.processed_books\n",
    "            eta_seconds = remaining / max(0.1, self.books_per_second)\n",
    "            eta_hours = eta_seconds / 3600\n",
    "            \n",
    "            elapsed_hours = (time.time() - self.start_time) / 3600\n",
    "            \n",
    "            msg = f\"üìä Progress: {progress_pct:.1f}% \"\n",
    "            msg += f\"({self.processed_books:,}/{self.total_books:,}) | \"\n",
    "            msg += f\"‚úÖ Success: {success_rate:.1f}% | \"\n",
    "            msg += f\"üöÄ Speed: {self.books_per_second:.1f} books/sec | \"\n",
    "            msg += f\"‚è±Ô∏è ETA: {eta_hours:.1f}h\"\n",
    "            \n",
    "            return msg\n",
    "\n",
    "class BERTCheckpointManager:\n",
    "    \"\"\"Checkpoint manager for BERT processing\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, 'bert_progress_checkpoint.json')\n",
    "        self.results_file = os.path.join(checkpoint_dir, 'bert_partial_results.pkl')\n",
    "        \n",
    "    def save_checkpoint(self, processed_indices, results, failed_books, progress_tracker):\n",
    "        \"\"\"Save checkpoint\"\"\"\n",
    "        checkpoint_data = {\n",
    "            'processed_indices': list(processed_indices),\n",
    "            'num_results': len(results),\n",
    "            'num_failed': len(failed_books),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'books_per_second': progress_tracker.books_per_second,\n",
    "            'success_rate': progress_tracker.successful_books / max(1, progress_tracker.processed_books),\n",
    "            'bert_version': True\n",
    "        }\n",
    "        \n",
    "        # Save checkpoint metadata\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "        \n",
    "        # Save actual results\n",
    "        with open(self.results_file, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'results': results,\n",
    "                'failed_books': failed_books\n",
    "            }, f)\n",
    "        \n",
    "        print(f\"üíæ BERT Checkpoint saved: {len(results):,} results, {len(failed_books):,} failed\")\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load checkpoint\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file) and os.path.exists(self.results_file):\n",
    "            try:\n",
    "                # Load metadata\n",
    "                with open(self.checkpoint_file, 'r') as f:\n",
    "                    checkpoint_data = json.load(f)\n",
    "                \n",
    "                # Load results\n",
    "                with open(self.results_file, 'rb') as f:\n",
    "                    saved_data = pickle.load(f)\n",
    "                \n",
    "                print(f\"üìÇ Loaded BERT checkpoint from {checkpoint_data['timestamp']}\")\n",
    "                print(f\"üìä Previous progress: {checkpoint_data['num_results']:,} results, {checkpoint_data['num_failed']:,} failed\")\n",
    "                \n",
    "                return (\n",
    "                    set(checkpoint_data['processed_indices']),\n",
    "                    saved_data['results'],\n",
    "                    saved_data['failed_books']\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading BERT checkpoint: {e}\")\n",
    "                return set(), [], []\n",
    "        \n",
    "        return set(), [], []\n",
    "\n",
    "class BERTEmotionPredictor:\n",
    "    \"\"\"BERT-based emotion predictor for Goodreads descriptions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.checkpoint_manager = BERTCheckpointManager(f\"{config.RESULTS_DIR}/checkpoints\")\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.progress_tracker = None\n",
    "        self.load_model()\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the fine-tuned BERT model and tokenizer\"\"\"\n",
    "        try:\n",
    "            print(f\"ü§ñ Loading fine-tuned BERT model from {config.BERT_MODEL_DIR}\")\n",
    "            \n",
    "            # Check if model directory exists\n",
    "            if not os.path.exists(config.BERT_MODEL_DIR):\n",
    "                print(f\"‚ùå Model directory not found: {config.BERT_MODEL_DIR}\")\n",
    "                print(\"Please ensure you have trained the model using utterance_to_emotion_with_transformer_my_try.py\")\n",
    "                print(\"And that the paths match your artemis directory structure\")\n",
    "                raise FileNotFoundError(f\"Model directory not found: {config.BERT_MODEL_DIR}\")\n",
    "            \n",
    "            # List files in model directory for debugging\n",
    "            model_files = os.listdir(config.BERT_MODEL_DIR)\n",
    "            print(f\"üìÅ Files in model directory: {model_files}\")\n",
    "            \n",
    "            # Load tokenizer and model\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(config.BERT_MODEL_DIR)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(config.BERT_MODEL_DIR)\n",
    "            \n",
    "            # Move to device\n",
    "            self.model.to(config.DEVICE)\n",
    "            self.model.eval()\n",
    "            \n",
    "            print(f\"‚úÖ BERT model loaded successfully\")\n",
    "            print(f\"üìä Model expects {self.model.config.num_labels} emotion classes\")\n",
    "            print(f\"üéØ Expected emotions: {len(ARTEMIS_EMOTIONS)}\")\n",
    "            \n",
    "            # Verify model configuration\n",
    "            if self.model.config.num_labels != len(ARTEMIS_EMOTIONS):\n",
    "                print(f\"‚ö†Ô∏è Warning: Model has {self.model.config.num_labels} labels, but ArtEmis has {len(ARTEMIS_EMOTIONS)} emotions\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading BERT model: {e}\")\n",
    "            print(\"Please ensure you have:\")\n",
    "            print(\"1. Trained the model using utterance_to_emotion_with_transformer_my_try.py\")\n",
    "            print(\"2. Set do_training=True in that script\")\n",
    "            print(\"3. Updated the paths to match your directory structure\")\n",
    "            raise\n",
    "    \n",
    "    def predict_emotions_batch(self, texts):\n",
    "        \"\"\"Predict emotions for a batch of texts\"\"\"\n",
    "        try:\n",
    "            # Tokenize texts (same parameters as training)\n",
    "            encodings = self.tokenizer(\n",
    "                texts,\n",
    "                truncation=True,\n",
    "                padding='max_length',  # Same as training\n",
    "                max_length=config.MAX_LENGTH,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            input_ids = encodings['input_ids'].to(config.DEVICE)\n",
    "            attention_mask = encodings['attention_mask'].to(config.DEVICE)\n",
    "            \n",
    "            # Predict\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Convert to probabilities\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                predictions = torch.argmax(probabilities, dim=-1)\n",
    "            \n",
    "            return predictions.cpu().numpy(), probabilities.cpu().numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in BERT prediction: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def process_batch(self, batch_df, description_field):\n",
    "        \"\"\"Process a batch of books\"\"\"\n",
    "        batch_results = []\n",
    "        batch_failed = []\n",
    "        \n",
    "        # Extract and preprocess descriptions\n",
    "        descriptions = []\n",
    "        book_data = []\n",
    "        \n",
    "        for idx, row in batch_df.iterrows():\n",
    "            description = row.get(description_field, '')\n",
    "            \n",
    "            if pd.isna(description) or str(description).strip() == '':\n",
    "                batch_failed.append({\n",
    "                    'book_id': row.get('book_id', idx),\n",
    "                    'reason': 'no_description'\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Preprocess description (minimal preprocessing to match training)\n",
    "            processed_desc = preprocess_text_artemis_style(description)\n",
    "            \n",
    "            if len(processed_desc.strip()) < 5:  # Very short descriptions\n",
    "                batch_failed.append({\n",
    "                    'book_id': row.get('book_id', idx),\n",
    "                    'reason': 'description_too_short'\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            descriptions.append(processed_desc)\n",
    "            book_data.append((idx, row))\n",
    "        \n",
    "        if not descriptions:\n",
    "            return batch_results, batch_failed\n",
    "        \n",
    "        # Process in smaller sub-batches for BERT (same batch size as training)\n",
    "        for i in range(0, len(descriptions), config.BATCH_SIZE):\n",
    "            sub_descriptions = descriptions[i:i+config.BATCH_SIZE]\n",
    "            sub_book_data = book_data[i:i+config.BATCH_SIZE]\n",
    "            \n",
    "            # Predict emotions\n",
    "            predictions, probabilities = self.predict_emotions_batch(sub_descriptions)\n",
    "            \n",
    "            if predictions is None:\n",
    "                # All failed in this sub-batch\n",
    "                for j, (idx, row) in enumerate(sub_book_data):\n",
    "                    batch_failed.append({\n",
    "                        'book_id': row.get('book_id', idx),\n",
    "                        'reason': 'prediction_failed'\n",
    "                    })\n",
    "                continue\n",
    "            \n",
    "            # Process results\n",
    "            for j, (idx, row) in enumerate(sub_book_data):\n",
    "                pred_idx = predictions[j]\n",
    "                emotion_probs = probabilities[j]\n",
    "                \n",
    "                # Get emotion label using IDX_TO_EMOTION (same as training)\n",
    "                predicted_emotion = IDX_TO_EMOTION[pred_idx]\n",
    "                confidence = emotion_probs[pred_idx]\n",
    "                \n",
    "                # Get positive/negative/else classification (same as training)\n",
    "                emotion_pne = positive_negative_else(predicted_emotion)\n",
    "                \n",
    "                result = {\n",
    "                    'book_id': row.get('book_id', idx),\n",
    "                    'title': row.get('title', ''),\n",
    "                    'authors': row.get('authors', ''),\n",
    "                    'average_rating': row.get('average_rating'),\n",
    "                    'ratings_count': row.get('ratings_count'),\n",
    "                    'publication_year': row.get('publication_year'),\n",
    "                    'language_code': row.get('language_code'),\n",
    "                    'popular_shelves': row.get('popular_shelves'),\n",
    "                    'description_preview': sub_descriptions[j][:200] + '...' if len(sub_descriptions[j]) > 200 else sub_descriptions[j],\n",
    "                    'description_length': len(sub_descriptions[j]),\n",
    "                    'predicted_emotion': predicted_emotion,\n",
    "                    'emotion_category': emotion_pne,\n",
    "                    'confidence': float(confidence),\n",
    "                    'emotion_probs': emotion_probs.tolist()\n",
    "                }\n",
    "                \n",
    "                # Add individual emotion probabilities (same as training order)\n",
    "                for k, emotion in enumerate(ARTEMIS_EMOTIONS):\n",
    "                    result[f'prob_{emotion.replace(\" \", \"_\")}'] = float(emotion_probs[k])\n",
    "                \n",
    "                batch_results.append(result)\n",
    "        \n",
    "        return batch_results, batch_failed\n",
    "    \n",
    "    def process_all_books(self, resume_from_checkpoint=True):\n",
    "        \"\"\"Main processing function\"\"\"\n",
    "        \n",
    "        print(\"üöÄ Starting BERT-based Goodreads emotion analysis...\")\n",
    "        print(f\"ü§ñ Using model from: {config.BERT_MODEL_DIR}\")\n",
    "        \n",
    "        # Load and filter data\n",
    "        print(\"üìÇ Loading Goodreads data...\")\n",
    "        try:\n",
    "            books_df = pd.read_parquet(config.DATA_FILE)\n",
    "            print(f\"üìä Loaded {len(books_df):,} total books\")\n",
    "            \n",
    "            # Filter for English books\n",
    "            books_df = filter_english_books(books_df)\n",
    "            \n",
    "            # Filter for books with descriptions\n",
    "            books_df, description_field = filter_books_with_descriptions(books_df)\n",
    "            \n",
    "            if description_field is None:\n",
    "                print(\"‚ùå No description field found. Cannot proceed.\")\n",
    "                return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Initialize progress tracking\n",
    "        self.progress_tracker = BERTProgressTracker(len(books_df))\n",
    "        \n",
    "        # Load previous progress if resuming\n",
    "        processed_indices = set()\n",
    "        all_results = []\n",
    "        all_failed = []\n",
    "        \n",
    "        if resume_from_checkpoint:\n",
    "            processed_indices, all_results, all_failed = self.checkpoint_manager.load_checkpoint()\n",
    "            if processed_indices:\n",
    "                print(f\"üîÑ Resuming from checkpoint: {len(processed_indices):,} books already processed\")\n",
    "                # Update progress tracker\n",
    "                self.progress_tracker.update(\n",
    "                    successful=len(all_results),\n",
    "                    failed=len(all_failed)\n",
    "                )\n",
    "        \n",
    "        # Filter out already processed books\n",
    "        remaining_books = books_df[~books_df.index.isin(processed_indices)].copy()\n",
    "        print(f\"üìã Processing {len(remaining_books):,} remaining English books with descriptions\")\n",
    "        \n",
    "        if len(remaining_books) == 0:\n",
    "            print(\"‚úÖ All books already processed!\")\n",
    "            return pd.DataFrame(all_results)\n",
    "        \n",
    "        try:\n",
    "            # Process in batches\n",
    "            total_batches = (len(remaining_books) + config.PROCESSING_BATCH_SIZE - 1) // config.PROCESSING_BATCH_SIZE\n",
    "            \n",
    "            print(f\"üîÑ Processing {total_batches} batches of up to {config.PROCESSING_BATCH_SIZE:,} books each\")\n",
    "            print(f\"üéØ Using BERT batch size of {config.BATCH_SIZE} for predictions\")\n",
    "            \n",
    "            # Create tqdm progress bar\n",
    "            batch_progress = tqdm(range(total_batches), desc=\"Processing BERT batches\")\n",
    "            \n",
    "            for batch_idx in batch_progress:\n",
    "                start_idx = batch_idx * config.PROCESSING_BATCH_SIZE\n",
    "                end_idx = min(start_idx + config.PROCESSING_BATCH_SIZE, len(remaining_books))\n",
    "                batch_df = remaining_books.iloc[start_idx:end_idx]\n",
    "                \n",
    "                # Process this batch\n",
    "                batch_results, batch_failed = self.process_batch(batch_df, description_field)\n",
    "                \n",
    "                # Update results\n",
    "                all_results.extend(batch_results)\n",
    "                all_failed.extend(batch_failed)\n",
    "                \n",
    "                # Update processed indices\n",
    "                for idx in batch_df.index:\n",
    "                    processed_indices.add(idx)\n",
    "                \n",
    "                # Update progress\n",
    "                self.progress_tracker.update(\n",
    "                    successful=len(batch_results),\n",
    "                    failed=len(batch_failed)\n",
    "                )\n",
    "                \n",
    "                # Update progress bar\n",
    "                success_rate = self.progress_tracker.successful_books / max(1, self.progress_tracker.processed_books) * 100\n",
    "                batch_progress.set_postfix({\n",
    "                    'Success Rate': f\"{success_rate:.1f}%\",\n",
    "                    'Speed': f\"{self.progress_tracker.books_per_second:.1f} books/sec\"\n",
    "                })\n",
    "                \n",
    "                # Log progress\n",
    "                if self.progress_tracker.should_log(config.PROGRESS_UPDATE_FREQUENCY):\n",
    "                    print(self.progress_tracker.get_status_message())\n",
    "                \n",
    "                # Save checkpoint\n",
    "                checkpoint_interval = max(1, config.CHECKPOINT_FREQUENCY // config.PROCESSING_BATCH_SIZE)\n",
    "                if (batch_idx + 1) % checkpoint_interval == 0:\n",
    "                    self.checkpoint_manager.save_checkpoint(\n",
    "                        processed_indices, all_results, all_failed, self.progress_tracker\n",
    "                    )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during processing: {e}\")\n",
    "            # Save emergency checkpoint\n",
    "            self.checkpoint_manager.save_checkpoint(\n",
    "                processed_indices, all_results, all_failed, self.progress_tracker\n",
    "            )\n",
    "            return None\n",
    "        \n",
    "        # Save final results\n",
    "        return self.save_final_results(all_results, all_failed)\n",
    "    \n",
    "    def save_final_results(self, results, failed):\n",
    "        \"\"\"Save final results\"\"\"\n",
    "        results_df = pd.DataFrame(results)\n",
    "        failed_df = pd.DataFrame(failed)\n",
    "        \n",
    "        # Save files\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results_file = f\"{config.RESULTS_DIR}/goodreads_bert_emotion_predictions_{timestamp}.parquet\"\n",
    "        failed_file = f\"{config.RESULTS_DIR}/goodreads_bert_failed_books_{timestamp}.csv\"\n",
    "        \n",
    "        results_df.to_parquet(results_file, index=False)\n",
    "        failed_df.to_csv(failed_file, index=False)\n",
    "        \n",
    "        # Clear checkpoint files\n",
    "        try:\n",
    "            if os.path.exists(self.checkpoint_manager.checkpoint_file):\n",
    "                os.remove(self.checkpoint_manager.checkpoint_file)\n",
    "            if os.path.exists(self.checkpoint_manager.results_file):\n",
    "                os.remove(self.checkpoint_manager.results_file)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Final statistics\n",
    "        total_processed = len(results) + len(failed)\n",
    "        success_rate = len(results) / total_processed * 100 if total_processed > 0 else 0\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üéâ BERT EMOTION ANALYSIS COMPLETED!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"üìä Total processed: {total_processed:,}\")\n",
    "        print(f\"‚úÖ Successful: {len(results):,} ({success_rate:.2f}%)\")\n",
    "        print(f\"‚ùå Failed: {len(failed):,}\")\n",
    "        print(f\"üíæ Results saved to: {results_file}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Emotion analysis\n",
    "        if len(results) > 0:\n",
    "            print(\"\\nüìà Emotion Distribution:\")\n",
    "            emotion_counts = results_df['predicted_emotion'].value_counts()\n",
    "            for emotion, count in emotion_counts.items():\n",
    "                pct = count / len(results_df) * 100\n",
    "                print(f\"  {emotion}: {count:,} ({pct:.1f}%)\")\n",
    "            \n",
    "            print(\"\\nüìä Emotion Categories (Positive/Negative/Else):\")\n",
    "            category_counts = results_df['emotion_category'].value_counts()\n",
    "            for category, count in category_counts.items():\n",
    "                pct = count / len(results_df) * 100\n",
    "                print(f\"  {category}: {count:,} ({pct:.1f}%)\")\n",
    "            \n",
    "            print(\"\\nüéØ Confidence Statistics:\")\n",
    "            print(f\"  Mean confidence: {results_df['confidence'].mean():.3f}\")\n",
    "            print(f\"  Median confidence: {results_df['confidence'].median():.3f}\")\n",
    "            high_conf = (results_df['confidence'] > 0.8).sum()\n",
    "            print(f\"  High confidence (>0.8): {high_conf:,} ({high_conf/len(results_df)*100:.1f}%)\")\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "# Main execution functions\n",
    "def run_bert_emotion_analysis(resume_from_checkpoint=True):\n",
    "    \"\"\"\n",
    "    Main function to run BERT emotion analysis\n",
    "    \n",
    "    Parameters:\n",
    "    - resume_from_checkpoint: Whether to resume from existing checkpoint (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ü§ñ BERT-based Goodreads Book Description Emotion Analysis\")\n",
    "    print(\"üìö Processing English books with descriptions only\")\n",
    "    print(\"üîÑ Using fine-tuned BERT model from ArtEmis training\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Check configuration\n",
    "    if not os.path.exists(config.BERT_MODEL_DIR):\n",
    "        print(f\"‚ùå BERT model directory not found: {config.BERT_MODEL_DIR}\")\n",
    "        print(\"Please ensure you have:\")\n",
    "        print(\"1. Trained the model using utterance_to_emotion_with_transformer_my_try.py\")\n",
    "        print(\"2. Set do_training=True in that script\")\n",
    "        print(\"3. Updated the path to match your artemis directory structure\")\n",
    "        \n",
    "        # Suggest alternative paths\n",
    "        possible_paths = [\n",
    "            r'artemis\\predictions\\bert_based\\best_model',\n",
    "            r'data\\artemis\\artemis\\predictions\\bert_based\\best_model',\n",
    "            r'predictions\\bert_based\\best_model'\n",
    "        ]\n",
    "        print(\"\\nPossible model paths to check:\")\n",
    "        for path in possible_paths:\n",
    "            print(f\"  - {path}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    if not os.path.exists(config.DATA_FILE):\n",
    "        print(f\"‚ùå Data file not found: {config.DATA_FILE}\")\n",
    "        print(\"Please update DATA_FILE in the config above\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize and run predictor\n",
    "    predictor = BERTEmotionPredictor()\n",
    "    results_df = predictor.process_all_books(resume_from_checkpoint)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Convenience function for one-line execution\n",
    "def analyze_goodreads_bert_emotions(resume_from_checkpoint=True):\n",
    "    \"\"\"\n",
    "    One-line function to run the complete BERT emotion analysis\n",
    "    \n",
    "    Usage:\n",
    "    results = analyze_goodreads_bert_emotions()\n",
    "    \"\"\"\n",
    "    return run_bert_emotion_analysis(resume_from_checkpoint)\n",
    "\n",
    "# Display configuration\n",
    "print(\"üìã BERT EMOTION ANALYSIS CONFIGURATION\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"üìÅ Artemis path: {config.ARTEMIS_PATH}\")\n",
    "print(f\"ü§ñ BERT model directory: {config.BERT_MODEL_DIR}\")\n",
    "print(f\"üìä Data file: {config.DATA_FILE}\")\n",
    "print(f\"üíæ Results directory: {config.RESULTS_DIR}\")\n",
    "print(f\"üñ•Ô∏è Device: {config.DEVICE}\")\n",
    "print(f\"üì¶ Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"üìè Max sequence length: {config.MAX_LENGTH}\")\n",
    "print(f\"üéØ Expected emotion classes: {len(ARTEMIS_EMOTIONS) if 'ARTEMIS_EMOTIONS' in globals() else 'Not loaded'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check if model exists and show status\n",
    "if os.path.exists(config.BERT_MODEL_DIR):\n",
    "    model_files = os.listdir(config.BERT_MODEL_DIR)\n",
    "    print(f\"‚úÖ Model directory found with files: {model_files}\")\n",
    "    print(\"Ready to run! Execute: results = analyze_goodreads_bert_emotions()\")\n",
    "else:\n",
    "    print(f\"‚ùå Model directory not found: {config.BERT_MODEL_DIR}\")\n",
    "    print(\"Please train the BERT model first using utterance_to_emotion_with_transformer_my_try.py\")\n",
    "\n",
    "results = analyze_goodreads_bert_emotions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c8fe0b-65cb-4a62-b259-0029d396f9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Creating comprehensive BERT emotion analysis visualizations...\n",
      "============================================================\n",
      "üé≠ BERT EMOTION ANALYSIS SUMMARY\n",
      "============================================================\n",
      "üìö Total books analyzed: 686,946\n",
      "üéØ Mean confidence: 0.481\n",
      "üìä Median confidence: 0.441\n",
      "üìè Average description length: 891 characters\n",
      "\n",
      "üé≠ EMOTION DISTRIBUTION:\n",
      "  sadness        : 192,494 ( 28.0%)\n",
      "  amusement      : 110,417 ( 16.1%)\n",
      "  excitement     : 100,331 ( 14.6%)\n",
      "  fear           : 94,141 ( 13.7%)\n",
      "  awe            : 89,756 ( 13.1%)\n",
      "  anger          : 35,642 (  5.2%)\n",
      "  contentment    : 26,742 (  3.9%)\n",
      "  something else : 19,989 (  2.9%)\n",
      "  disgust        : 17,434 (  2.5%)\n",
      "\n",
      "üìà EMOTION CATEGORIES:\n",
      "                1: 339,711 ( 49.5%)\n",
      "                0: 327,246 ( 47.6%)\n",
      "                2: 19,989 (  2.9%)\n",
      "\n",
      "üéØ CONFIDENCE ANALYSIS:\n",
      "  Confidence ‚â• 0.5: 265,905 ( 38.7%)\n",
      "  Confidence ‚â• 0.6: 169,487 ( 24.7%)\n",
      "  Confidence ‚â• 0.7: 102,818 ( 15.0%)\n",
      "  Confidence ‚â• 0.8: 54,237 (  7.9%)\n",
      "  Confidence ‚â• 0.9: 18,565 (  2.7%)\n",
      "\n",
      "üèÜ MOST CONFIDENT PREDICTIONS:\n",
      "  amusement    (0.994): Zombie High Yearbook '64...\n",
      "  amusement    (0.994): Buck's Tooth...\n",
      "  amusement    (0.993): The Adventures of George...\n",
      "  amusement    (0.993): Exhaust(ed): The 99% True Story of a Bus Trip Gone...\n",
      "  amusement    (0.993): Cowpoke Clyde and Dirty Dawg...\n",
      "============================================================\n",
      "üìä Comprehensive visualization saved to goodreads_bert_emotion_results/visualizations/bert_emotion_analysis_comprehensive.png\n"
     ]
    }
   ],
   "source": [
    "# BERT Emotion Analysis Visualization Functions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_bert_visualizations(results_df, save_dir=None):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for BERT emotion analysis results\n",
    "    \n",
    "    Parameters:\n",
    "    - results_df: DataFrame with BERT emotion predictions\n",
    "    - save_dir: Directory to save plots (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    if results_df is None or len(results_df) == 0:\n",
    "        print(\"‚ùå No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 1. Emotion Distribution (Bar Plot)\n",
    "    plt.subplot(3, 3, 1)\n",
    "    emotion_counts = results_df['predicted_emotion'].value_counts()\n",
    "    bars = plt.bar(range(len(emotion_counts)), emotion_counts.values, alpha=0.8)\n",
    "    plt.xticks(range(len(emotion_counts)), emotion_counts.index, rotation=45, ha='right')\n",
    "    plt.title('Emotion Distribution in Goodreads Books\\n(BERT Predictions)', fontsize=12, pad=20)\n",
    "    plt.ylabel('Number of Books')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, value) in enumerate(zip(bars, emotion_counts.values)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, \n",
    "                f'{value:,}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 2. Emotion Categories Pie Chart\n",
    "    plt.subplot(3, 3, 2)\n",
    "    category_counts = results_df['emotion_category'].value_counts()\n",
    "    colors = ['#2ecc71', '#e74c3c', '#95a5a6']  # Green, Red, Gray\n",
    "    wedges, texts, autotexts = plt.pie(category_counts.values, labels=category_counts.index, \n",
    "                                      autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    plt.title('Emotion Categories Distribution\\n(Positive/Negative/Else)', fontsize=12, pad=20)\n",
    "    \n",
    "    # Enhance pie chart text\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    # 3. Confidence Distribution Histogram\n",
    "    plt.subplot(3, 3, 3)\n",
    "    plt.hist(results_df['confidence'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(results_df['confidence'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {results_df[\"confidence\"].mean():.3f}')\n",
    "    plt.axvline(results_df['confidence'].median(), color='orange', linestyle='--', \n",
    "                label=f'Median: {results_df[\"confidence\"].median():.3f}')\n",
    "    plt.title('Prediction Confidence Distribution', fontsize=12, pad=20)\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Number of Books')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Average Confidence by Emotion\n",
    "    plt.subplot(3, 3, 4)\n",
    "    emotion_confidence = results_df.groupby('predicted_emotion')['confidence'].agg(['mean', 'std']).sort_values('mean', ascending=False)\n",
    "    bars = plt.bar(range(len(emotion_confidence)), emotion_confidence['mean'].values, \n",
    "                   yerr=emotion_confidence['std'].values, alpha=0.8, capsize=5)\n",
    "    plt.xticks(range(len(emotion_confidence)), emotion_confidence.index, rotation=45, ha='right')\n",
    "    plt.title('Average Prediction Confidence by Emotion', fontsize=12, pad=20)\n",
    "    plt.ylabel('Average Confidence')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, value) in enumerate(zip(bars, emotion_confidence['mean'].values)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 5. Confidence vs Emotion Category\n",
    "    plt.subplot(3, 3, 5)\n",
    "    categories = results_df['emotion_category'].unique()\n",
    "    confidence_by_category = [results_df[results_df['emotion_category'] == cat]['confidence'].values \n",
    "                             for cat in categories]\n",
    "    \n",
    "    box_plot = plt.boxplot(confidence_by_category, labels=categories, patch_artist=True)\n",
    "    colors = ['lightgreen', 'lightcoral', 'lightgray']\n",
    "    for patch, color in zip(box_plot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    plt.title('Confidence Distribution by Emotion Category', fontsize=12, pad=20)\n",
    "    plt.ylabel('Confidence Score')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 6. Description Length vs Confidence\n",
    "    plt.subplot(3, 3, 6)\n",
    "    plt.scatter(results_df['description_length'], results_df['confidence'], alpha=0.5, s=10)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(results_df['description_length'], results_df['confidence'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(results_df['description_length'], p(results_df['description_length']), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.title('Description Length vs Prediction Confidence', fontsize=12, pad=20)\n",
    "    plt.xlabel('Description Length (characters)')\n",
    "    plt.ylabel('Confidence Score')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = results_df['description_length'].corr(results_df['confidence'])\n",
    "    plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    # 7. High Confidence Predictions by Emotion\n",
    "    plt.subplot(3, 3, 7)\n",
    "    high_conf_threshold = 0.8\n",
    "    high_conf_emotions = results_df[results_df['confidence'] > high_conf_threshold]['predicted_emotion'].value_counts()\n",
    "    \n",
    "    if len(high_conf_emotions) > 0:\n",
    "        bars = plt.bar(range(len(high_conf_emotions)), high_conf_emotions.values, alpha=0.8, color='gold')\n",
    "        plt.xticks(range(len(high_conf_emotions)), high_conf_emotions.index, rotation=45, ha='right')\n",
    "        plt.title(f'High Confidence Predictions (>{high_conf_threshold})', fontsize=12, pad=20)\n",
    "        plt.ylabel('Number of Books')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add percentages\n",
    "        total_high_conf = len(results_df[results_df['confidence'] > high_conf_threshold])\n",
    "        for i, (bar, value) in enumerate(zip(bars, high_conf_emotions.values)):\n",
    "            pct = value / total_high_conf * 100\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "                    f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No high confidence predictions', ha='center', va='center', \n",
    "                transform=plt.gca().transAxes, fontsize=12)\n",
    "        plt.title(f'High Confidence Predictions (>{high_conf_threshold})', fontsize=12, pad=20)\n",
    "    \n",
    "    # 8. Emotion Probability Heatmap (Top emotions)\n",
    "    plt.subplot(3, 3, 8)\n",
    "    \n",
    "    # Get top 6 most frequent emotions for readability\n",
    "    top_emotions = emotion_counts.head(6).index\n",
    "    prob_columns = [f'prob_{emotion.replace(\" \", \"_\")}' for emotion in top_emotions]\n",
    "    \n",
    "    # Sample data for heatmap (use first 100 books or less)\n",
    "    sample_size = min(100, len(results_df))\n",
    "    sample_df = results_df.head(sample_size)\n",
    "    prob_matrix = sample_df[prob_columns].values.T\n",
    "    \n",
    "    im = plt.imshow(prob_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    plt.colorbar(im, shrink=0.8)\n",
    "    plt.yticks(range(len(top_emotions)), top_emotions)\n",
    "    plt.xlabel('Book Samples')\n",
    "    plt.title(f'Emotion Probability Heatmap\\n(First {sample_size} books)', fontsize=12, pad=20)\n",
    "    \n",
    "    # 9. Summary Statistics Table\n",
    "    plt.subplot(3, 3, 9)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    stats = {\n",
    "        'Total Books Analyzed': f\"{len(results_df):,}\",\n",
    "        'Mean Confidence': f\"{results_df['confidence'].mean():.3f}\",\n",
    "        'Median Confidence': f\"{results_df['confidence'].median():.3f}\",\n",
    "        'High Confidence (>0.8)': f\"{(results_df['confidence'] > 0.8).sum():,} ({(results_df['confidence'] > 0.8).mean()*100:.1f}%)\",\n",
    "        'Most Common Emotion': f\"{emotion_counts.index[0]} ({emotion_counts.iloc[0]:,})\",\n",
    "        'Positive Emotions': f\"{(results_df['emotion_category'] == 'positive').sum():,} ({(results_df['emotion_category'] == 'positive').mean()*100:.1f}%)\",\n",
    "        'Negative Emotions': f\"{(results_df['emotion_category'] == 'negative').sum():,} ({(results_df['emotion_category'] == 'negative').mean()*100:.1f}%)\",\n",
    "        'Avg Description Length': f\"{results_df['description_length'].mean():.0f} chars\"\n",
    "    }\n",
    "    \n",
    "    # Create table\n",
    "    table_data = [[key, value] for key, value in stats.items()]\n",
    "    table = plt.table(cellText=table_data, colLabels=['Metric', 'Value'],\n",
    "                     cellLoc='left', loc='center', bbox=[0, 0, 1, 1])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 2)\n",
    "    \n",
    "    # Style the table\n",
    "    for i in range(len(stats) + 1):\n",
    "        table[(i, 0)].set_facecolor('#E8E8E8')\n",
    "        table[(i, 1)].set_facecolor('#F5F5F5')\n",
    "    \n",
    "    plt.title('Summary Statistics', fontsize=12, pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot if directory provided\n",
    "    if save_dir:\n",
    "        import os\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plt.savefig(f'{save_dir}/bert_emotion_analysis_comprehensive.png', \n",
    "                   dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"üìä Comprehensive visualization saved to {save_dir}/bert_emotion_analysis_comprehensive.png\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Additional detailed visualizations\n",
    "    create_detailed_emotion_analysis(results_df, save_dir)\n",
    "\n",
    "def create_detailed_emotion_analysis(results_df, save_dir=None):\n",
    "    \"\"\"Create additional detailed analysis plots\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Confidence distribution by emotion (violin plot)\n",
    "    ax1 = axes[0, 0]\n",
    "    emotions = results_df['predicted_emotion'].unique()\n",
    "    confidence_data = [results_df[results_df['predicted_emotion'] == emotion]['confidence'].values \n",
    "                      for emotion in emotions]\n",
    "    \n",
    "    parts = ax1.violinplot(confidence_data, positions=range(len(emotions)), showmeans=True)\n",
    "    ax1.set_xticks(range(len(emotions)))\n",
    "    ax1.set_xticklabels(emotions, rotation=45, ha='right')\n",
    "    ax1.set_title('Confidence Distribution by Emotion (Violin Plot)')\n",
    "    ax1.set_ylabel('Confidence Score')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Emotion transitions/correlations heatmap\n",
    "    ax2 = axes[0, 1]\n",
    "    from artemis.emotions import ARTEMIS_EMOTIONS\n",
    "    \n",
    "    # Create correlation matrix of emotion probabilities\n",
    "    prob_columns = [f'prob_{emotion.replace(\" \", \"_\")}' for emotion in ARTEMIS_EMOTIONS]\n",
    "    corr_matrix = results_df[prob_columns].corr()\n",
    "    \n",
    "    im = ax2.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    ax2.set_xticks(range(len(ARTEMIS_EMOTIONS)))\n",
    "    ax2.set_yticks(range(len(ARTEMIS_EMOTIONS)))\n",
    "    ax2.set_xticklabels(ARTEMIS_EMOTIONS, rotation=45, ha='right')\n",
    "    ax2.set_yticklabels(ARTEMIS_EMOTIONS)\n",
    "    ax2.set_title('Emotion Probability Correlations')\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(len(ARTEMIS_EMOTIONS)):\n",
    "        for j in range(len(ARTEMIS_EMOTIONS)):\n",
    "            text = ax2.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\" if abs(corr_matrix.iloc[i, j]) < 0.5 else \"white\")\n",
    "    \n",
    "    plt.colorbar(im, ax=ax2, shrink=0.8)\n",
    "    \n",
    "    # 3. Description length distribution by emotion category\n",
    "    ax3 = axes[1, 0]\n",
    "    categories = results_df['emotion_category'].unique()\n",
    "    length_data = [results_df[results_df['emotion_category'] == cat]['description_length'].values \n",
    "                  for cat in categories]\n",
    "    \n",
    "    box_plot = ax3.boxplot(length_data, labels=categories, patch_artist=True)\n",
    "    colors = ['lightgreen', 'lightcoral', 'lightgray']\n",
    "    for patch, color in zip(box_plot['boxes'], colors[:len(categories)]):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    ax3.set_title('Description Length by Emotion Category')\n",
    "    ax3.set_ylabel('Description Length (characters)')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Confidence threshold analysis\n",
    "    ax4 = axes[1, 1]\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    coverage = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        high_conf_mask = results_df['confidence'] >= threshold\n",
    "        coverage.append(high_conf_mask.mean() * 100)\n",
    "        \n",
    "        # For accuracy, we'd need ground truth labels\n",
    "        # For now, just show coverage\n",
    "    \n",
    "    ax4.plot(thresholds, coverage, 'b-', linewidth=2, label='Coverage %')\n",
    "    ax4.set_xlabel('Confidence Threshold')\n",
    "    ax4.set_ylabel('Coverage (%)')\n",
    "    ax4.set_title('Coverage vs Confidence Threshold')\n",
    "    ax4.grid(alpha=0.3)\n",
    "    ax4.legend()\n",
    "    \n",
    "    # Add some key points\n",
    "    for i, (thresh, cov) in enumerate(zip(thresholds[::5], coverage[::5])):\n",
    "        ax4.annotate(f'{cov:.1f}%', (thresh, cov), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        plt.savefig(f'{save_dir}/bert_detailed_analysis.png', \n",
    "                   dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"üìä Detailed analysis saved to {save_dir}/bert_detailed_analysis.png\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def print_emotion_summary(results_df):\n",
    "    \"\"\"Print a detailed text summary of the emotion analysis\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üé≠ BERT EMOTION ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"üìö Total books analyzed: {len(results_df):,}\")\n",
    "    print(f\"üéØ Mean confidence: {results_df['confidence'].mean():.3f}\")\n",
    "    print(f\"üìä Median confidence: {results_df['confidence'].median():.3f}\")\n",
    "    print(f\"üìè Average description length: {results_df['description_length'].mean():.0f} characters\")\n",
    "    \n",
    "    # Emotion distribution\n",
    "    print(f\"\\nüé≠ EMOTION DISTRIBUTION:\")\n",
    "    emotion_counts = results_df['predicted_emotion'].value_counts()\n",
    "    for emotion, count in emotion_counts.items():\n",
    "        pct = count / len(results_df) * 100\n",
    "        print(f\"  {emotion:15}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Category distribution\n",
    "    print(f\"\\nüìà EMOTION CATEGORIES:\")\n",
    "    category_counts = results_df['emotion_category'].value_counts()\n",
    "    for category, count in category_counts.items():\n",
    "        pct = count / len(results_df) * 100\n",
    "        print(f\"  {category:15}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Confidence analysis\n",
    "    print(f\"\\nüéØ CONFIDENCE ANALYSIS:\")\n",
    "    conf_thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    for threshold in conf_thresholds:\n",
    "        high_conf = (results_df['confidence'] >= threshold).sum()\n",
    "        pct = high_conf / len(results_df) * 100\n",
    "        print(f\"  Confidence ‚â• {threshold}: {high_conf:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Most confident predictions\n",
    "    print(f\"\\nüèÜ MOST CONFIDENT PREDICTIONS:\")\n",
    "    top_confident = results_df.nlargest(5, 'confidence')[['title', 'predicted_emotion', 'confidence']]\n",
    "    for _, row in top_confident.iterrows():\n",
    "        print(f\"  {row['predicted_emotion']:12} ({row['confidence']:.3f}): {row['title'][:50]}...\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Usage function to run all visualizations\n",
    "def analyze_bert_results(results_df, save_dir=None):\n",
    "    \"\"\"\n",
    "    Run complete analysis and visualization of BERT results\n",
    "    \n",
    "    Usage after running BERT analysis:\n",
    "    analyze_bert_results(results, 'visualizations')\n",
    "    \"\"\"\n",
    "    \n",
    "    if save_dir is None:\n",
    "        save_dir = config.RESULTS_DIR if 'config' in globals() else 'bert_visualizations'\n",
    "    \n",
    "    print(\"üé® Creating comprehensive BERT emotion analysis visualizations...\")\n",
    "    \n",
    "    # Print text summary\n",
    "    print_emotion_summary(results_df)\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_bert_visualizations(results_df, save_dir)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Analysis complete! Visualizations saved to: {save_dir}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example usage:\n",
    "# After running your BERT analysis:\n",
    "# results = analyze_goodreads_bert_emotions()\n",
    "\n",
    "vis_dir = f\"{config.RESULTS_DIR}/visualizations\"\n",
    "os.makedirs(vis_dir, exist_ok=True)\n",
    "analyze_bert_results(results, vis_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c21a42-7bae-448e-b500-93f2dcf9ec59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
