{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki aesthetics images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aesthetic</th>\n",
       "      <th>image_count</th>\n",
       "      <th>total_size_mb</th>\n",
       "      <th>avg_size_mb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Frogcore</td>\n",
       "      <td>182</td>\n",
       "      <td>35.86</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kidcore</td>\n",
       "      <td>75</td>\n",
       "      <td>28.39</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dark_Academia</td>\n",
       "      <td>63</td>\n",
       "      <td>17.28</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fairy_Kei</td>\n",
       "      <td>60</td>\n",
       "      <td>7.77</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Traumacore</td>\n",
       "      <td>59</td>\n",
       "      <td>19.03</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cottagecore</td>\n",
       "      <td>55</td>\n",
       "      <td>21.11</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ethereal</td>\n",
       "      <td>50</td>\n",
       "      <td>12.76</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Vaporwave</td>\n",
       "      <td>47</td>\n",
       "      <td>44.19</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bloomcore</td>\n",
       "      <td>40</td>\n",
       "      <td>11.29</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cyberpunk</td>\n",
       "      <td>33</td>\n",
       "      <td>28.30</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Pastel_Goth</td>\n",
       "      <td>30</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Princesscore</td>\n",
       "      <td>29</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hellenic</td>\n",
       "      <td>28</td>\n",
       "      <td>28.37</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Vibrant_Academia</td>\n",
       "      <td>22</td>\n",
       "      <td>7.97</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Goblincore</td>\n",
       "      <td>21</td>\n",
       "      <td>9.14</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Light_Academia</td>\n",
       "      <td>14</td>\n",
       "      <td>6.89</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angura_Kei</td>\n",
       "      <td>14</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Virgo%27s_Tears</td>\n",
       "      <td>12</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Atompunk</td>\n",
       "      <td>11</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Monkeycore</td>\n",
       "      <td>10</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anglo_Gothic</td>\n",
       "      <td>8</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bubblegum_Witch</td>\n",
       "      <td>7</td>\n",
       "      <td>8.04</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Grandparentcore</td>\n",
       "      <td>7</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fashwave</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>VaporGoth</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           aesthetic  image_count  total_size_mb  avg_size_mb\n",
       "11          Frogcore          182          35.86         0.20\n",
       "15           Kidcore           75          28.39         0.38\n",
       "7      Dark_Academia           63          17.28         0.27\n",
       "9          Fairy_Kei           60           7.77         0.13\n",
       "20        Traumacore           59          19.03         0.32\n",
       "5        Cottagecore           55          21.11         0.38\n",
       "8           Ethereal           50          12.76         0.26\n",
       "22         Vaporwave           47          44.19         0.94\n",
       "3          Bloomcore           40          11.29         0.28\n",
       "6          Cyberpunk           33          28.30         0.86\n",
       "18       Pastel_Goth           30           3.21         0.11\n",
       "19      Princesscore           29           6.60         0.23\n",
       "14          Hellenic           28          28.37         1.01\n",
       "23  Vibrant_Academia           22           7.97         0.36\n",
       "12        Goblincore           21           9.14         0.44\n",
       "16    Light_Academia           14           6.89         0.49\n",
       "1         Angura_Kei           14           4.13         0.30\n",
       "24   Virgo%27s_Tears           12           2.46         0.21\n",
       "2           Atompunk           11           2.07         0.19\n",
       "17        Monkeycore           10           2.34         0.23\n",
       "0       Anglo_Gothic            8           0.27         0.03\n",
       "4    Bubblegum_Witch            7           8.04         1.15\n",
       "13   Grandparentcore            7           0.35         0.05\n",
       "10          Fashwave            0           0.00         0.00\n",
       "21         VaporGoth            0           0.00         0.00"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the base path\n",
    "base_path = \"data/aesthetic_images/\"\n",
    "\n",
    "# Get list of all aesthetic folders\n",
    "aesthetic_folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "\n",
    "# Create a list to store the counts\n",
    "counts = []\n",
    "\n",
    "# Count files in each folder and get additional statistics\n",
    "for aesthetic in aesthetic_folders:\n",
    "    folder_path = os.path.join(base_path, aesthetic)\n",
    "    image_files = glob.glob(os.path.join(folder_path, \"*\"))\n",
    "    \n",
    "    # Calculate total size in MB\n",
    "    total_size_bytes = sum(os.path.getsize(file) for file in image_files)\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "    \n",
    "    counts.append({\n",
    "        \"aesthetic\": aesthetic,\n",
    "        \"image_count\": len(image_files),\n",
    "        \"total_size_mb\": round(total_size_mb, 2),\n",
    "        \"avg_size_mb\": round(total_size_mb / len(image_files), 2) if image_files else 0\n",
    "    })\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df_image_counts = pd.DataFrame(counts)\n",
    "\n",
    "# Sort by image count (descending)\n",
    "df_image_counts = df_image_counts.sort_values(\"image_count\", ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_image_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodreads data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..\\\\goodreads\\\\goodreads_books\\\\goodreads_books.json.gz', '..\\\\goodreads\\\\goodreads_books\\\\goodreads_book_authors.json.gz', '..\\\\goodreads\\\\goodreads_books\\\\goodreads_book_genres_initial.json.gz', '..\\\\goodreads\\\\goodreads_books\\\\goodreads_book_series.json.gz', '..\\\\goodreads\\\\goodreads_books\\\\goodreads_book_works.json.gz']\n"
     ]
    }
   ],
   "source": [
    "BOOKS_PATH = r'..\\goodreads\\goodreads_books'\n",
    "\n",
    "# All book datasets\n",
    "book_files = glob.glob(os.path.join(BOOKS_PATH, \"*.gz\"))\n",
    "\n",
    "print(book_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a08fe0926a45519e23204383e729db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 16 with 100,000 records to ../goodreads/goodreads_chunks/books_chunk_016.csv\n",
      "Saved chunk 17 with 100,000 records to ../goodreads/goodreads_chunks/books_chunk_017.csv\n",
      "Saved chunk 18 with 100,000 records to ../goodreads/goodreads_chunks/books_chunk_018.csv\n",
      "Saved chunk 19 with 100,000 records to ../goodreads/goodreads_chunks/books_chunk_019.csv\n",
      "Saved chunk 20 with 100,000 records to ../goodreads/goodreads_chunks/books_chunk_020.csv\n",
      "Saved chunk 21 with 100,000 records to ../goodreads/goodreads_chunks/books_chunk_021.csv\n",
      "Saved chunk 22 with 100,000 records to ../goodreads/goodreads_chunks/books_chunk_022.csv\n",
      "Saved final chunk 23 with 60,656 records to ../goodreads/goodreads_chunks/books_chunk_023.csv\n"
     ]
    }
   ],
   "source": [
    "def process_and_save_chunks(file_path, output_dir, chunk_number=0, chunk_size=50000, max_chunks=None):\n",
    "    \"\"\"\n",
    "    Process large JSON.GZ file in chunks and save each chunk as a CSV\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the input JSON.GZ file\n",
    "    output_dir : str\n",
    "        Directory to save the CSV chunks\n",
    "    chunk_size : int\n",
    "        Number of records per chunk\n",
    "    max_chunks : int, optional\n",
    "        Maximum number of chunks to process\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in tqdm(enumerate(f)):\n",
    "            if i <= chunk_number * chunk_size:\n",
    "                continue\n",
    "            try:\n",
    "                # Parse JSON line\n",
    "                book = json.loads(line.strip())\n",
    "                records.append(book)\n",
    "                \n",
    "                # Process chunk when it reaches the specified size\n",
    "                if len(records) >= chunk_size:\n",
    "                    chunk_df = pd.DataFrame(records)\n",
    "                    \n",
    "                    # Save this chunk\n",
    "                    chunk_file = os.path.join(output_dir, f\"books_chunk_{chunk_number:03d}.csv\")\n",
    "                    chunk_df.to_csv(chunk_file, index=False)\n",
    "                    print(f\"Saved chunk {chunk_number} with {len(records):,} records to {chunk_file}\")\n",
    "                    \n",
    "                    # Reset for next chunk\n",
    "                    records = []\n",
    "                    chunk_number += 1\n",
    "                    \n",
    "                    # Stop if we reached max_chunks\n",
    "                    if max_chunks and chunk_number >= max_chunks:\n",
    "                        print(f\"Reached maximum number of chunks ({max_chunks})\")\n",
    "                        break\n",
    "                        \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error parsing JSON at line {i}\")\n",
    "    \n",
    "    # Save any remaining records\n",
    "    if records:\n",
    "        chunk_df = pd.DataFrame(records)\n",
    "        chunk_file = os.path.join(output_dir, f\"books_chunk_{chunk_number:03d}.csv\")\n",
    "        chunk_df.to_csv(chunk_file, index=False)\n",
    "        print(f\"Saved final chunk {chunk_number} with {len(records):,} records to {chunk_file}\")\n",
    "\n",
    "\n",
    "# process_and_save_chunks(\n",
    "#     '../goodreads/goodreads_books/goodreads_books.json.gz', \n",
    "#     '../goodreads/goodreads_chunks/',\n",
    "#     chunk_size=100000,\n",
    "#     chunk_number=16,\n",
    "#     max_chunks=24 \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pyarrow'.  Use pip or conda to install pyarrow.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Friso\\OneDrive - UvA\\Data Science\\Thesis\\Code\\thesis\\Lib\\site-packages\\dask\\_compatibility.py:114\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, min_version, errors)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\Anaconda\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Read all CSV files into a Dask DataFrame\u001b[39;00m\n\u001b[0;32m      4\u001b[0m dask_df \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../goodreads/goodreads_chunks/books_chunk_*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Friso\\OneDrive - UvA\\Data Science\\Thesis\\Code\\thesis\\Lib\\site-packages\\dask\\dataframe\\__init__.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_collection_type\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backends, dispatch\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdask_expr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     DataFrame,\n\u001b[0;32m     27\u001b[0m     Index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     to_timedelta,\n\u001b[0;32m     54\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Aggregation\n",
      "File \u001b[1;32mc:\\Users\\Friso\\OneDrive - UvA\\Data Science\\Thesis\\Code\\thesis\\Lib\\site-packages\\dask\\dataframe\\backends.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpercentile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _percentile\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CreationDispatch, DaskBackendEntrypoint\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PANDAS_GE_220, is_any_real_numeric_dtype\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     categorical_dtype_dispatch,\n\u001b[0;32m     17\u001b[0m     concat,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     union_categoricals_dispatch,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_array_nonempty, make_scalar\n",
      "File \u001b[1;32mc:\\Users\\Friso\\OneDrive - UvA\\Data Science\\Thesis\\Code\\thesis\\Lib\\site-packages\\dask\\dataframe\\_compat.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Friso\\OneDrive - UvA\\Data Science\\Thesis\\Code\\thesis\\Lib\\site-packages\\dask\\_compatibility.py:117\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, min_version, errors)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'pyarrow'.  Use pip or conda to install pyarrow."
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read all CSV files into a Dask DataFrame\n",
    "dask_df = dd.read_csv('../goodreads/goodreads_chunks/books_chunk_*.csv')\n",
    "\n",
    "# Now you can work with this DataFrame much like pandas\n",
    "# Basic operations are lazy and only computed when needed\n",
    "print(f\"Total rows: {len(dask_df):,}\")\n",
    "\n",
    "# Get basic statistics (triggers computation)\n",
    "stats = dask_df.describe().compute()\n",
    "display(stats)\n",
    "\n",
    "# Example: Get most common publishers\n",
    "top_publishers = dask_df['publisher'].value_counts().nlargest(20).compute()\n",
    "display(top_publishers)\n",
    "\n",
    "# Example: Average rating by year\n",
    "avg_by_year = dask_df.groupby('publication_year')['average_rating'].mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp; Company</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN                                         Book-Title  \\\n",
       "0  0195153448                                Classical Mythology   \n",
       "1  0002005018                                       Clara Callan   \n",
       "2  0060973129                               Decision in Normandy   \n",
       "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4  0393045218                             The Mummies of Urumchi   \n",
       "\n",
       "            Book-Author  Year-Of-Publication                Publisher  \\\n",
       "0    Mark P. O. Morford                 2002  Oxford University Press   \n",
       "1  Richard Bruce Wright                 2001    HarperFlamingo Canada   \n",
       "2          Carlo D'Este                 1991          HarperPerennial   \n",
       "3      Gina Bari Kolata                 1999     Farrar Straus Giroux   \n",
       "4       E. J. W. Barber                 1999   W. W. Norton & Company   \n",
       "\n",
       "                                         Image-URL-S  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-M  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-L  \n",
       "0  http://images.amazon.com/images/P/0195153448.0...  \n",
       "1  http://images.amazon.com/images/P/0002005018.0...  \n",
       "2  http://images.amazon.com/images/P/0060973129.0...  \n",
       "3  http://images.amazon.com/images/P/0374157065.0...  \n",
       "4  http://images.amazon.com/images/P/0393045218.0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(271379, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "book_crossing = pd.read_csv(r'..\\bookcrossing\\Book reviews\\Book reviews\\BX_Books.csv', sep=';', encoding='latin-1')\n",
    "display(book_crossing.head(), book_crossing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
