{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96e9047e-ab02-4f15-8b58-9ad602ae2627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading EmotionCLIP-V2...\n",
      "EmotionCLIP-V2 loaded successfully\n",
      "Base EmotionCLIP model frozen\n",
      "Loading data...\n",
      "Loading ArtEmis data...\n",
      "Original dataset size: 79388\n",
      "After removing NaN: 79388\n",
      "After checking image files: 79388\n",
      "Train: 50 samples\n",
      "Val: 50 samples\n",
      "Test: 50 samples\n",
      "Initializing fine-tuned model...\n",
      "Initialized vision adapter with dimension 512\n",
      "Creating data loaders...\n",
      "Trainable parameters: 513\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 3/12 [00:00<00:01,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in image features, using random features\n",
      "NaN/Inf in logits batch 0, skipping\n",
      "NaN in image features, using random features\n",
      "NaN/Inf in logits batch 1, skipping\n",
      "NaN in image features, using random features\n",
      "NaN/Inf in logits batch 2, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 5/12 [00:00<00:00,  8.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in image features, using random features\n",
      "NaN/Inf in logits batch 3, skipping\n",
      "NaN in image features, using random features\n",
      "NaN/Inf in logits batch 4, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 8/12 [00:00<00:00, 11.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in image features, using random features\n",
      "NaN/Inf in logits batch 5, skipping\n",
      "NaN in image features, using random features\n",
      "NaN/Inf in logits batch 6, skipping\n",
      "NaN in image features, using random features\n",
      "NaN/Inf in logits batch 7, skipping\n",
      "NaN in image features, using random features\n",
      "NaN/Inf in logits batch 8, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 10/12 [00:01<00:00, 12.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in image features, using random features\n",
      "NaN/Inf in logits batch 9, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 12/12 [00:01<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in image features, using random features\n",
      "NaN/Inf in logits batch 10, skipping\n",
      "NaN in image features, using random features\n",
      "NaN/Inf in logits batch 11, skipping\n",
      "Train Loss: inf, Train Accuracy: 0.0000\n",
      "Training failed due to NaN/Inf loss. Stopping...\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   8%|▊         | 1/13 [00:00<00:05,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in image features, using random features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  31%|███       | 4/13 [00:00<00:01,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in image features, using random features\n",
      "NaN in image features, using random features\n",
      "NaN in image features, using random features\n",
      "NaN in image features, using random features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  54%|█████▍    | 7/13 [00:00<00:00,  9.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in image features, using random features\n",
      "NaN in image features, using random features\n",
      "NaN in image features, using random features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  77%|███████▋  | 10/13 [00:01<00:00, 12.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in image features, using random features\n",
      "NaN in image features, using random features\n",
      "NaN in image features, using random features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in image features, using random features\n",
      "NaN in image features, using random features\n",
      "\n",
      "Test Results:\n",
      "Test Loss: inf\n",
      "Test Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EmotionCLIP Fine-tuning for ArtEmis Emotion Classification - FIXED VERSION\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "from PIL import Image\n",
    "import unicodedata\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Import EmotionCLIP-V2\n",
    "from EmotionCLIP_V2 import EmotionCLIP\n",
    "\n",
    "# Import artemis modules\n",
    "from artemis.emotions import ARTEMIS_EMOTIONS, IDX_TO_EMOTION, positive_negative_else\n",
    "from artemis.in_out.basics import create_dir\n",
    "\n",
    "# Configuration\n",
    "num_labels = len(ARTEMIS_EMOTIONS)\n",
    "model_name = 'EmotionCLIP-V2'\n",
    "load_best_model = False\n",
    "do_training = True\n",
    "max_train_epochs = 5  # Reduced for testing\n",
    "subsample_data = True  # Enable for faster testing\n",
    "\n",
    "# Fine-tuning method selection\n",
    "use_layer_norm_tuning = False  # Disabled for now\n",
    "use_prefix_tuning = True\n",
    "use_prompt_tuning = False  # Disabled to simplify\n",
    "\n",
    "# Paths (matching your structure)\n",
    "my_out_dir = r'finetuned_emotionclip_results/emotionclip_based'\n",
    "artemis_preprocessed_dir = r'data/artemis/old_preprocessed_data'\n",
    "image_hists_file = r'data/artemis/artemis/data/image-emotion-histogram.csv'\n",
    "\n",
    "create_dir(my_out_dir)\n",
    "best_model_dir = osp.join(my_out_dir, 'best_model')\n",
    "create_dir(best_model_dir)\n",
    "\n",
    "# Training parameters - CONSERVATIVE SETTINGS\n",
    "batch_size = 4  # Very small batch size\n",
    "learning_rate = 1e-6  # Very small learning rate\n",
    "weight_decay = 0.01\n",
    "max_grad_norm = 0.5  # Aggressive gradient clipping\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fine-tuning parameters - REDUCED\n",
    "prefix_length = 2  # Very small\n",
    "prefix_dim = 128  # Reduced\n",
    "\n",
    "# Emotion label mapping\n",
    "emotion_mapping = {emotion: idx for idx, emotion in enumerate(ARTEMIS_EMOTIONS)}\n",
    "\n",
    "class ArtemisEmotionDataset(Dataset):\n",
    "    \"\"\"Dataset class for ArtEmis emotion data\"\"\"\n",
    "    \n",
    "    def __init__(self, df, preprocess, emotion_mapping):\n",
    "        self.df = df\n",
    "        self.preprocess = preprocess\n",
    "        self.emotion_mapping = emotion_mapping\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            img_path = row['image_file']\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                return None\n",
    "            \n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image_tensor = self.preprocess(image)\n",
    "            \n",
    "            # Get emotion distribution\n",
    "            emotion_dist = row['emotion_distribution']\n",
    "            \n",
    "            if isinstance(emotion_dist, str):\n",
    "                emotion_dist = literal_eval(emotion_dist)\n",
    "            \n",
    "            emotion_dist = np.array(emotion_dist, dtype=np.float32)\n",
    "            \n",
    "            # Ensure valid distribution\n",
    "            if emotion_dist.sum() <= 0:\n",
    "                emotion_dist = np.ones(len(self.emotion_mapping)) / len(self.emotion_mapping)\n",
    "            else:\n",
    "                emotion_dist = emotion_dist / emotion_dist.sum()\n",
    "            \n",
    "            emotion_dist = torch.tensor(emotion_dist, dtype=torch.float32)\n",
    "            \n",
    "            # Get dominant emotion\n",
    "            dominant_emotion_idx = torch.argmax(emotion_dist).item()\n",
    "            \n",
    "            return {\n",
    "                'image': image_tensor,\n",
    "                'emotion_distribution': emotion_dist,\n",
    "                'emotion_label': dominant_emotion_idx,\n",
    "                'image_path': img_path\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {row.get('image_file', 'unknown')}: {e}\")\n",
    "            return None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    \n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    emotion_distributions = torch.stack([item['emotion_distribution'] for item in batch])\n",
    "    emotion_labels = torch.tensor([item['emotion_label'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'emotion_distributions': emotion_distributions,\n",
    "        'emotion_labels': emotion_labels,\n",
    "        'image_paths': [item['image_path'] for item in batch]\n",
    "    }\n",
    "\n",
    "class SimpleEmotionCLIPTuning(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified EmotionCLIP fine-tuning - FIXED VERSION\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, clip_model, emotion_mapping):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.emotion_mapping = emotion_mapping\n",
    "        self.num_emotions = len(emotion_mapping)\n",
    "        \n",
    "        # Get model dimensions safely\n",
    "        self.feature_dim = self._get_feature_dimension()\n",
    "        \n",
    "        # Create text prompts\n",
    "        self.emotion_texts = [f\"This image shows {emotion}\" for emotion in emotion_mapping.keys()]\n",
    "        \n",
    "        # ONLY use simple prefix tuning\n",
    "        if use_prefix_tuning:\n",
    "            self.vision_adapter = nn.Parameter(\n",
    "                torch.zeros(self.feature_dim) * 0.01  # Very small initialization\n",
    "            )\n",
    "            print(f\"Initialized vision adapter with dimension {self.feature_dim}\")\n",
    "        else:\n",
    "            self.vision_adapter = None\n",
    "        \n",
    "        # Simple learnable temperature\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n",
    "        \n",
    "    def _get_feature_dimension(self):\n",
    "        \"\"\"Get feature dimension safely\"\"\"\n",
    "        try:\n",
    "            dummy_image = torch.randn(1, 3, 224, 224).to(device)\n",
    "            with torch.no_grad():\n",
    "                features = self.clip_model.encode_image(dummy_image.to(self.clip_model.dtype))\n",
    "                return features.shape[-1]\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting feature dimension: {e}\")\n",
    "            return 512  # Default fallback\n",
    "    \n",
    "    def encode_text_prompts(self):\n",
    "        \"\"\"Encode text prompts once and cache\"\"\"\n",
    "        if not hasattr(self, '_cached_text_features'):\n",
    "            try:\n",
    "                # Use EmotionCLIP tokenizer\n",
    "                text_tokens = EmotionCLIP.tokenizer(self.emotion_texts).to(device)\n",
    "                with torch.no_grad():\n",
    "                    text_features = self.clip_model.encode_text(text_tokens)\n",
    "                    # Normalize\n",
    "                    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "                    self._cached_text_features = text_features.detach()\n",
    "            except Exception as e:\n",
    "                print(f\"Error encoding text: {e}\")\n",
    "                # Fallback: random features\n",
    "                self._cached_text_features = torch.randn(self.num_emotions, self.feature_dim, dtype=torch.float32).to(device)\n",
    "                self._cached_text_features = self._cached_text_features / self._cached_text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return self._cached_text_features\n",
    "    \n",
    "    def forward(self, images):\n",
    "        \"\"\"Simple forward pass\"\"\"\n",
    "        try:\n",
    "            # Encode images - ENSURE GRADIENTS FLOW\n",
    "            if use_prefix_tuning and self.vision_adapter is not None:\n",
    "                # Enable gradients for vision adapter\n",
    "                image_features = self.clip_model.encode_image(images.to(self.clip_model.dtype))\n",
    "                \n",
    "                # Apply simple adapter\n",
    "                image_features = image_features + self.vision_adapter.unsqueeze(0)\n",
    "            else:\n",
    "                # No fine-tuning, just use base model\n",
    "                with torch.no_grad():\n",
    "                    image_features = self.clip_model.encode_image(images.to(self.clip_model.dtype))\n",
    "                # Convert to require grad for loss computation\n",
    "                image_features = image_features.detach().requires_grad_(True)\n",
    "            \n",
    "            # Check for NaN\n",
    "            if torch.isnan(image_features).any():\n",
    "                print(\"NaN in image features, using random features\")\n",
    "                image_features = torch.randn_like(image_features).to(image_features.dtype).requires_grad_(True)      \n",
    "            \n",
    "            # Normalize image features\n",
    "            image_features = image_features / (image_features.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "            \n",
    "            # Get text features (cached) - FIX: Ensure same dtype\n",
    "            text_features = self.encode_text_prompts().to(image_features.dtype)\n",
    "            \n",
    "            # Compute logits - FIX: Ensure consistent dtype\n",
    "            logit_scale = torch.clamp(self.logit_scale.exp(), min=1.0, max=100.0).to(image_features.dtype)\n",
    "            logits = logit_scale * image_features @ text_features.t()\n",
    "            \n",
    "            # Clamp logits to prevent overflow\n",
    "            logits = torch.clamp(logits, min=-20, max=20)\n",
    "            \n",
    "            return logits\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in forward pass: {e}\")\n",
    "            # Return dummy logits that require grad\n",
    "            batch_size = images.size(0)\n",
    "            dummy_logits = torch.randn(batch_size, self.num_emotions, requires_grad=True).to(device)\n",
    "            return dummy_logits\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and prepare the ArtEmis dataset\"\"\"\n",
    "    print(\"Loading ArtEmis data...\")\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    artemis_data = pd.read_csv(osp.join(artemis_preprocessed_dir, 'artemis_preprocessed.csv'))\n",
    "    artemis_data['image_file'] = artemis_data['image_file'].apply(lambda x: unicodedata.normalize('NFC', x))\n",
    "    artemis_data['painting'] = artemis_data['painting'].apply(lambda x: unicodedata.normalize('NFC', x))\n",
    "    \n",
    "    # Keep each image once\n",
    "    artemis_data = artemis_data.drop_duplicates(subset=['art_style', 'painting'])\n",
    "    artemis_data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Load emotion histograms\n",
    "    try:\n",
    "        image_hists = pd.read_csv(image_hists_file)\n",
    "        image_hists.emotion_histogram = image_hists.emotion_histogram.apply(literal_eval)\n",
    "        image_hists.emotion_histogram = image_hists.emotion_histogram.apply(\n",
    "            lambda x: (np.array(x) / max(sum(x), 1e-8)).astype('float32')\n",
    "        )\n",
    "        \n",
    "        artemis_data = artemis_data[['art_style', 'painting', 'split', 'image_file']]\n",
    "        artemis_data = artemis_data.merge(image_hists, on=['art_style', 'painting'], how='inner')\n",
    "        artemis_data = artemis_data.rename(columns={'emotion_histogram': 'emotion_distribution'})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading emotion histograms: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    # Validate data\n",
    "    print(f\"Original dataset size: {len(artemis_data)}\")\n",
    "    artemis_data = artemis_data.dropna(subset=['emotion_distribution', 'image_file'])\n",
    "    print(f\"After removing NaN: {len(artemis_data)}\")\n",
    "    \n",
    "    # Check image files exist\n",
    "    valid_images = artemis_data['image_file'].apply(os.path.exists)\n",
    "    artemis_data = artemis_data[valid_images]\n",
    "    print(f\"After checking image files: {len(artemis_data)}\")\n",
    "    \n",
    "    # Create data splits\n",
    "    data_splits = {}\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        mask = (artemis_data['split'] == split)\n",
    "        sub_df = artemis_data[mask].copy()\n",
    "        sub_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        if len(sub_df) == 0:\n",
    "            print(f\"Warning: No data found for split '{split}'\")\n",
    "            continue\n",
    "        \n",
    "        if subsample_data:\n",
    "            sample_size = min(50, len(sub_df))  # Very small for testing\n",
    "            sub_df = sub_df.sample(sample_size)\n",
    "            sub_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        data_splits[split] = sub_df\n",
    "        print(f\"{split.capitalize()}: {len(sub_df)} samples\")\n",
    "    \n",
    "    return data_splits\n",
    "\n",
    "def create_data_loaders(data_splits, preprocess):\n",
    "    \"\"\"Create PyTorch DataLoaders\"\"\"\n",
    "    datasets = {}\n",
    "    data_loaders = {}\n",
    "    \n",
    "    for split_name, split_data in data_splits.items():\n",
    "        dataset = ArtemisEmotionDataset(\n",
    "            split_data, \n",
    "            preprocess, \n",
    "            emotion_mapping\n",
    "        )\n",
    "        datasets[split_name] = dataset\n",
    "        \n",
    "        shuffle = (split_name == 'train')\n",
    "        data_loaders[split_name] = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle,\n",
    "            num_workers=0,  # No multiprocessing\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=(split_name == 'train')\n",
    "        )\n",
    "    \n",
    "    return data_loaders, datasets\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    valid_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            images = batch['images'].to(device)\n",
    "            emotion_distributions = batch['emotion_distributions'].to(device)\n",
    "            emotion_labels = batch['emotion_labels'].to(device)\n",
    "            \n",
    "            # Validate inputs\n",
    "            if torch.isnan(images).any() or torch.isnan(emotion_distributions).any():\n",
    "                print(f\"NaN in batch {batch_idx}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(images)\n",
    "            \n",
    "            # Check outputs\n",
    "            if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "                print(f\"NaN/Inf in logits batch {batch_idx}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Compute loss\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            loss = criterion(log_probs, emotion_distributions)\n",
    "            \n",
    "            if torch.isnan(loss) or torch.isinf(loss) or loss.item() > 50:\n",
    "                print(f\"Invalid loss {loss.item()} in batch {batch_idx}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                [p for p in model.parameters() if p.requires_grad], \n",
    "                max_grad_norm\n",
    "            )\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            valid_batches += 1\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            correct_predictions += (predictions == emotion_labels).sum().item()\n",
    "            total_predictions += emotion_labels.size(0)\n",
    "            \n",
    "            if total_predictions > 0:\n",
    "                accuracy = correct_predictions / total_predictions\n",
    "                avg_loss = total_loss / valid_batches\n",
    "                \n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': avg_loss,\n",
    "                    'accuracy': accuracy,\n",
    "                    'valid_batches': valid_batches\n",
    "                })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in training batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_loss = total_loss / valid_batches if valid_batches > 0 else float('inf')\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    valid_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"Evaluating\")):\n",
    "            if batch is None:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                images = batch['images'].to(device)\n",
    "                emotion_distributions = batch['emotion_distributions'].to(device)\n",
    "                emotion_labels = batch['emotion_labels'].to(device)\n",
    "                \n",
    "                if torch.isnan(images).any() or torch.isnan(emotion_distributions).any():\n",
    "                    continue\n",
    "                \n",
    "                logits = model(images)\n",
    "                \n",
    "                if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "                    continue\n",
    "                \n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "                loss = criterion(log_probs, emotion_distributions)\n",
    "                \n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    continue\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                valid_batches += 1\n",
    "                \n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(emotion_labels.cpu().numpy())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in evaluation batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    avg_loss = total_loss / valid_batches if valid_batches > 0 else float('inf')\n",
    "    accuracy = accuracy_score(all_labels, all_predictions) if all_labels else 0\n",
    "    \n",
    "    return avg_loss, accuracy, np.array(all_predictions), np.array(all_labels)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    # Setup\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load EmotionCLIP model\n",
    "    print(\"Loading EmotionCLIP-V2...\")\n",
    "    try:\n",
    "        clip_model = EmotionCLIP.model.to(device)\n",
    "        preprocess = EmotionCLIP.preprocess\n",
    "        print(\"EmotionCLIP-V2 loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading EmotionCLIP: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Freeze base model\n",
    "    for param in clip_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    print(\"Base EmotionCLIP model frozen\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    data_splits = load_data()\n",
    "    \n",
    "    if len(data_splits) == 0:\n",
    "        print(\"Error: No data splits found!\")\n",
    "        return\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"Initializing fine-tuned model...\")\n",
    "    model = SimpleEmotionCLIPTuning(clip_model, emotion_mapping)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create data loaders\n",
    "    print(\"Creating data loaders...\")\n",
    "    data_loaders, datasets = create_data_loaders(data_splits, preprocess)\n",
    "    \n",
    "    # Setup training\n",
    "    trainable_params = []\n",
    "    if use_prefix_tuning and model.vision_adapter is not None:\n",
    "        trainable_params.append(model.vision_adapter)\n",
    "    trainable_params.append(model.logit_scale)\n",
    "    \n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params)}\")\n",
    "    \n",
    "    if do_training and len(trainable_params) > 0:\n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "        optimizer = optim.Adam(trainable_params, lr=learning_rate, weight_decay=weight_decay)\n",
    "        criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "        \n",
    "        best_val_accuracy = 0\n",
    "        \n",
    "        for epoch in range(max_train_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{max_train_epochs}\")\n",
    "            \n",
    "            # Training\n",
    "            train_loss, train_accuracy = train_epoch(\n",
    "                model, data_loaders['train'], optimizer, criterion, device\n",
    "            )\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "            \n",
    "            if np.isnan(train_loss) or np.isinf(train_loss):\n",
    "                print(\"Training failed due to NaN/Inf loss. Stopping...\")\n",
    "                break\n",
    "            \n",
    "            # Validation\n",
    "            if 'val' in data_loaders:\n",
    "                val_loss, val_accuracy, _, _ = evaluate(model, data_loaders['val'], criterion, device)\n",
    "                print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "                \n",
    "                # Save best model\n",
    "                if val_accuracy > best_val_accuracy:\n",
    "                    best_val_accuracy = val_accuracy\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'val_accuracy': val_accuracy,\n",
    "                    }, osp.join(best_model_dir, 'model.pt'))\n",
    "                    print(f\"New best model saved: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    if 'test' in data_loaders:\n",
    "        print(\"Evaluating on test set...\")\n",
    "        criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "        test_loss, test_accuracy, predictions, labels = evaluate(\n",
    "            model, data_loaders['test'], criterion, device\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTest Results:\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "        if len(predictions) > 0 and len(labels) > 0:\n",
    "            # Ternary analysis\n",
    "            gt = pd.Series(labels)\n",
    "            predictions_series = pd.Series(predictions)\n",
    "            \n",
    "            gt_pne = gt.apply(lambda x: positive_negative_else(IDX_TO_EMOTION[x]))\n",
    "            predictions_pne = predictions_series.apply(lambda x: positive_negative_else(IDX_TO_EMOTION[x]))\n",
    "            \n",
    "            ternary_accuracy = (gt_pne == predictions_pne).mean()\n",
    "            print(f'Ternary accuracy (pos/neg/else): {ternary_accuracy:.4f}')\n",
    "            \n",
    "            # Save results\n",
    "            results = {\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'test_loss': test_loss,\n",
    "                'ternary_accuracy': ternary_accuracy,\n",
    "            }\n",
    "            \n",
    "            results_df = pd.DataFrame([results])\n",
    "            results_df.to_csv(osp.join(my_out_dir, 'test_results.csv'), index=False)\n",
    "            print(f\"Results saved to {my_out_dir}\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae2a3e-9da7-4efd-ad4d-9101ad0f4dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
