{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "895b9870-5ea7-4342-baa6-92ea3477f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from helpers import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6c529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ARTEMIS_PATH = \"data/artemis-v2/dataset/combined/train/artemis_preprocessed.csv\"\n",
    "# WIKIART_ROOT = \"data/wikiart_extracted/\"\n",
    "\n",
    "# Subset configuration\n",
    "USE_SUBSET = False        # Set to True to use a subset, False to use all train/val data\n",
    "SUBSET_FRACTION = 0.05     # Fraction of the combined train+val data to use (e.g., 0.1 for 10%)\n",
    "\n",
    "# Training configuration\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "RANDOM_SEED = 6552\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CURRENT_TIME = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bc8d786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 475996 annotations.\n",
      "Found 9 unique emotion classes: ['amusement', 'anger', 'awe', 'contentment', 'disgust', 'excitement', 'fear', 'sadness', 'something else']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b237f235da5c4761825166ab480e82b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping Images:   0%|          | 0/80317 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created multi-label data for 80317 unique images across splits.\n",
      "Unique images per split: Train=69529, Val=3598, Test=7190\n",
      "Using full unique image sets for training and validation.\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED) \n",
    "\n",
    "df = pd.read_csv(ARTEMIS_PATH)\n",
    "\n",
    "print(f\"Loaded {len(df)} annotations.\")\n",
    "\n",
    "df['image_file'] = df['image_file'].apply(lambda x: unicodedata.normalize('NFC', x))\n",
    "\n",
    "# Create integer labels for emotions\n",
    "emotions = sorted(df['emotion'].unique())\n",
    "emotion_to_idx = {emotion: i for i, emotion in enumerate(emotions)}\n",
    "idx_to_emotion = {i: emotion for emotion, i in emotion_to_idx.items()}\n",
    "num_classes = len(emotions)\n",
    "print(f\"Found {num_classes} unique emotion classes: {emotions}\")\n",
    "\n",
    "# Add integer label column\n",
    "df['emotion_idx'] = df['emotion'].map(emotion_to_idx)\n",
    "\n",
    "# Group by the unique image path and the original split\n",
    "grouped = df.groupby(['image_file', 'split'])\n",
    "\n",
    "multi_label_data = []\n",
    "for (img_path, split), group in tqdm(grouped, desc=\"Grouping Images\"):\n",
    "    # Get all unique emotion indices for this image within this split\n",
    "    unique_emotion_indices = group['emotion_idx'].unique()\n",
    "\n",
    "    # Create multi-hot encoded vector\n",
    "    label_vector = torch.zeros(num_classes, dtype=torch.float32)\n",
    "    label_vector[unique_emotion_indices] = 1.0\n",
    "\n",
    "    multi_label_data.append({\n",
    "        'image_file': img_path,\n",
    "        'split': split,\n",
    "        'multi_hot_label': label_vector\n",
    "    })\n",
    "\n",
    "df_unique_images = pd.DataFrame(multi_label_data)\n",
    "print(f\"Created multi-label data for {len(df_unique_images)} unique images across splits.\")\n",
    "\n",
    "# --- 5. Split Unique Image Data ---\n",
    "train_df_full = df_unique_images[df_unique_images['split'] == 'train'].reset_index(drop=True)\n",
    "val_df_full   = df_unique_images[df_unique_images['split'] == 'val'].reset_index(drop=True)\n",
    "test_df       = df_unique_images[df_unique_images['split'] == 'test'].reset_index(drop=True) # Keep test set separate\n",
    "\n",
    "print(f\"Unique images per split: Train={len(train_df_full)}, Val={len(val_df_full)}, Test={len(test_df)}\")\n",
    "\n",
    "\n",
    "# --- 6. Subset Selection (Random Sample of Unique Images) ---\n",
    "if USE_SUBSET and SUBSET_FRACTION > 0 and SUBSET_FRACTION < 1:\n",
    "    print(f\"Selecting a random {SUBSET_FRACTION:.0%} subset of UNIQUE train/val images...\")\n",
    "\n",
    "    # Combine unique train and validation images\n",
    "    train_val_unique_full = pd.concat([train_df_full, val_df_full], ignore_index=True)\n",
    "\n",
    "    # Sample a fraction of the unique images randomly\n",
    "    # NOTE: Stratification on multi-hot labels is complex, doing random sample for simplicity.\n",
    "    subset_df = train_val_unique_full.sample(frac=SUBSET_FRACTION, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    print(f\"Total unique images in subset: {len(subset_df)}\")\n",
    "\n",
    "    # Split this SUBSET back into train and validation (simple split, no stratification here)\n",
    "    # Calculate the original train/(train+val) ratio based on *unique image counts*\n",
    "    if (len(train_df_full) + len(val_df_full)) > 0:\n",
    "       original_train_ratio = len(train_df_full) / (len(train_df_full) + len(val_df_full))\n",
    "    else:\n",
    "       original_train_ratio = 1.0 # Default if no val data\n",
    "\n",
    "    if len(subset_df) < 2 or original_train_ratio >=1 or original_train_ratio <=0 :\n",
    "         print(\"Warning: Subset too small or original ratio invalid for train/val split. Using entire subset for training.\")\n",
    "         train_df = subset_df\n",
    "         val_df = pd.DataFrame(columns=subset_df.columns) # Empty df\n",
    "    else:\n",
    "        # Use train_test_split just for index splitting\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            subset_df.index,\n",
    "            train_size=original_train_ratio,\n",
    "            random_state=RANDOM_SEED\n",
    "            # No stratification here for simplicity\n",
    "        )\n",
    "        train_df = subset_df.loc[train_indices].reset_index(drop=True)\n",
    "        val_df = subset_df.loc[val_indices].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Unique images in subset splits: Train={len(train_df)}, Val={len(val_df)}\")\n",
    "\n",
    "else:\n",
    "    print(\"Using full unique image sets for training and validation.\")\n",
    "    train_df = train_df_full\n",
    "    val_df = val_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8239278-61d4-43b2-8e53-d4e70b661d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute path not found: data/wikiart_extracted/Naive_Art_Primitivism/grã©goire-michonze_figures-in-the-village(1).jpg\n"
     ]
    }
   ],
   "source": [
    "# Example with an absolute path (replace with your actual path)\n",
    "absolute_image_path = r\"\\data\\wikiart_extracted\\Naive_Art_Primitivism\\grã©goire-michonze_figures-in-the-village(1).jpg\"\n",
    "absolute_image_path = \"data/wikiart_extracted/Naive_Art_Primitivism/grã©goire-michonze_figures-in-the-village(1).jpg\"\n",
    "\n",
    "if os.path.exists(absolute_image_path):\n",
    "    img = Image.open(absolute_image_path)\n",
    "    print(\"Opened with absolute path\")\n",
    "else:\n",
    "    print(f\"Absolute path not found: {absolute_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78238cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataLoaders...\n",
      "Setting up model...\n",
      "\n",
      "Starting training for 20 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arch/RHEL8/EB_production/2023/software/Pillow/10.0.0-GCCcore-12.3.0/lib/python3.11/site-packages/PIL/Image.py:3157: DecompressionBombWarning: Image size (107327830 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/sw/arch/RHEL8/EB_production/2023/software/Pillow/10.0.0-GCCcore-12.3.0/lib/python3.11/site-packages/PIL/Image.py:3157: DecompressionBombWarning: Image size (99962094 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.5866 - Val Loss: 0.6100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arch/RHEL8/EB_production/2023/software/Pillow/10.0.0-GCCcore-12.3.0/lib/python3.11/site-packages/PIL/Image.py:3157: DecompressionBombWarning: Image size (107327830 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/sw/arch/RHEL8/EB_production/2023/software/Pillow/10.0.0-GCCcore-12.3.0/lib/python3.11/site-packages/PIL/Image.py:3157: DecompressionBombWarning: Image size (99962094 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train Loss: 0.5808 - Val Loss: 0.5989\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arch/RHEL8/EB_production/2023/software/Pillow/10.0.0-GCCcore-12.3.0/lib/python3.11/site-packages/PIL/Image.py:3157: DecompressionBombWarning: Image size (99962094 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train Loss: 0.5781 - Val Loss: 0.6065\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arch/RHEL8/EB_production/2023/software/Pillow/10.0.0-GCCcore-12.3.0/lib/python3.11/site-packages/PIL/Image.py:3157: DecompressionBombWarning: Image size (107327830 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train Loss: 0.5769 - Val Loss: 0.5979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train Loss: 0.5747 - Val Loss: 0.5984\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train Loss: 0.5731 - Val Loss: 0.5939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arch/RHEL8/EB_production/2023/software/Pillow/10.0.0-GCCcore-12.3.0/lib/python3.11/site-packages/PIL/Image.py:3157: DecompressionBombWarning: Image size (107327830 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/sw/arch/RHEL8/EB_production/2023/software/Pillow/10.0.0-GCCcore-12.3.0/lib/python3.11/site-packages/PIL/Image.py:3157: DecompressionBombWarning: Image size (99962094 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train Loss: 0.5717 - Val Loss: 0.5940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train Loss: 0.5703 - Val Loss: 0.5940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train Loss: 0.5689 - Val Loss: 0.5960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train Loss: 0.5672 - Val Loss: 0.5885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train Loss: 0.5661 - Val Loss: 0.5891\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train Loss: 0.5646 - Val Loss: 0.5952\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train Loss: 0.5634 - Val Loss: 0.5963\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train Loss: 0.5616 - Val Loss: 0.5932\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Train Loss: 0.5603 - Val Loss: 0.5900\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Train Loss: 0.5588 - Val Loss: 0.5996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Train Loss: 0.5571 - Val Loss: 0.5931\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Train Loss: 0.5557 - Val Loss: 0.5993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Train Loss: 0.5537 - Val Loss: 0.5943\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20 [Val]:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Train Loss: 0.5525 - Val Loss: 0.5983\n",
      "\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "class MultiLabelImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame with unique images.\n",
    "                                      Must have 'image_file' and 'multi_hot_label'.\n",
    "            transform (callable, optional): Transform to apply to the image.\n",
    "        \"\"\"\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "        if 'image_file' not in self.df.columns or 'multi_hot_label' not in self.df.columns:\n",
    "             raise ValueError(\"DataFrame must contain 'image_file' and 'multi_hot_label'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.loc[idx, 'image_file']\n",
    "        # The label is already a pre-computed tensor\n",
    "        label = self.df.loc[idx, 'multi_hot_label']\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Image file not found: {img_path}\")\n",
    "            return None, None # Indicate error\n",
    "        except Exception as e:\n",
    "             print(f\"ERROR: Could not load/process image {img_path}: {e}\")\n",
    "             return None, None # Indicate error\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Label is already a tensor, just return it\n",
    "        return image, label\n",
    "\n",
    "# --- 8. Transforms (Same as before) ---\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# --- 9. DataLoaders (Same collate_fn) ---\n",
    "print(\"Creating DataLoaders...\")\n",
    "train_ds = MultiLabelImageDataset(train_df, transform=train_tf)\n",
    "# Only create val_ds if val_df is not empty\n",
    "val_ds = None\n",
    "if not val_df.empty:\n",
    "    val_ds = MultiLabelImageDataset(val_df, transform=val_tf)\n",
    "\n",
    "\n",
    "def collate_fn_skip_error(batch):\n",
    "    batch = list(filter(lambda x: x is not None and x[0] is not None and x[1] is not None, batch))\n",
    "    if not batch:\n",
    "        return None, None\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=False, collate_fn=collate_fn_skip_error, persistent_workers=True if NUM_WORKERS > 0 else False) # pin_memory=False for CPU usually\n",
    "\n",
    "val_loader = None\n",
    "if val_ds:\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=False, collate_fn=collate_fn_skip_error, persistent_workers=True if NUM_WORKERS > 0 else False)\n",
    "else:\n",
    "    print(\"Validation set is empty, skipping validation loader creation.\")\n",
    "\n",
    "\n",
    "# --- 10. Model Definition (Output layer unchanged, interpretation changes) ---\n",
    "print(\"Setting up model...\")\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "in_features = model.fc.in_features\n",
    "# Output layer still has num_classes neurons, one for each potential emotion label\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "model = model.to(DEVICE) # Moves model to CPU\n",
    "\n",
    "# --- 11. Loss Function and Optimizer (CHANGED FOR MULTI-LABEL) ---\n",
    "# BCEWithLogitsLoss is standard for multi-label problems with multi-hot targets\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- 12. Training Loop (Adapted for Multi-Label) ---\n",
    "print(f\"\\nStarting training for {NUM_EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    # Accuracy calculation is less straightforward for multi-label.\n",
    "    # We can track loss, or use metrics like Hamming Loss or threshold-based accuracy later.\n",
    "    # Let's focus on loss for the training printout.\n",
    "    processed_samples_train = 0 # Keep track of successfully processed samples\n",
    "\n",
    "    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False)\n",
    "    for imgs, labels in train_loop:\n",
    "        if imgs is None or labels is None: continue # Skip batches with loading errors\n",
    "\n",
    "        # Labels are already float tensors (multi-hot) from the dataset\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(imgs) # Raw logits from the model\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "        processed_samples_train += imgs.size(0)\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = train_loss / processed_samples_train if processed_samples_train > 0 else 0\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    avg_val_loss = float('nan')\n",
    "    processed_samples_val = 0\n",
    "    if val_loader:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\", leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loop:\n",
    "                if imgs is None or labels is None: continue\n",
    "\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels) # Use the same multi-label loss\n",
    "\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "                processed_samples_val += imgs.size(0)\n",
    "                val_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_val_loss = val_loss / processed_samples_val if processed_samples_val > 0 else 0\n",
    "\n",
    "    # --- Log Epoch Results (Focus on Loss) ---\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} - \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "          # Add multi-label accuracy metrics here if desired later\n",
    "\n",
    "    # Optional: Save checkpoint\n",
    "    torch.save(model.state_dict(), f'wikiart_emotion_ml_epoch_{epoch+1}.pth')\n",
    "\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), f'wikiart_emotion_ml_{CURRENT_TIME}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e9e3725-5a2d-4fb7-9fe4-1915a5fcd2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test loader with 7190 images\n",
      "\n",
      "Evaluating model on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577ad86486cd42be85e50297280458c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Model Evaluation Results =====\n",
      "Hamming Loss: 0.3121 (lower is better)\n",
      "Exact Match Accuracy: 0.0441\n",
      "Micro-F1 Score: 0.4879\n",
      "Macro-F1 Score: 0.3650\n",
      "Micro-Precision: 0.6482\n",
      "Micro-Recall: 0.3912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/fharlaar.11868458/ipykernel_1634912/3359438716.py:113: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to evaluation_results_2025-05-15_14-06-46.csv\n",
      "Predictions saved to predictions_2025-05-15_14-06-46.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/fharlaar.11868458/ipykernel_1634912/3359438716.py:131: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluation Loop with Multi-Label Metrics ---\n",
    "from sklearn.metrics import hamming_loss, f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, data_loader, threshold=0.5, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Evaluate a trained multi-label classification model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        data_loader: DataLoader with test/validation data\n",
    "        threshold: Probability threshold for positive prediction (default: 0.5)\n",
    "        device: Device to run evaluation on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with various evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            if images is None or labels is None:\n",
    "                continue\n",
    "                \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs >= threshold).float()\n",
    "            \n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_predictions.append(preds.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    \n",
    "    # Concatenate batch results\n",
    "    y_true = np.vstack(all_labels)\n",
    "    y_pred = np.vstack(all_predictions)\n",
    "    y_prob = np.vstack(all_probs)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        'hamming_loss': hamming_loss(y_true, y_pred),\n",
    "        'exact_match': accuracy_score(y_true, y_pred),  # All labels must match\n",
    "        'f1_micro': f1_score(y_true, y_pred, average='micro'),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'f1_samples': f1_score(y_true, y_pred, average='samples'),\n",
    "        'precision_micro': precision_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "        'recall_micro': recall_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "    }\n",
    "    \n",
    "    # Add per-class metrics\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        metrics[f'precision_{emotion}'] = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        metrics[f'recall_{emotion}'] = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        metrics[f'f1_{emotion}'] = f1_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        \n",
    "        # ROC AUC if we have both positive and negative samples\n",
    "        if len(np.unique(y_true[:, i])) > 1:\n",
    "            metrics[f'auc_{emotion}'] = roc_auc_score(y_true[:, i], y_prob[:, i])\n",
    "        else:\n",
    "            metrics[f'auc_{emotion}'] = float('nan')\n",
    "    \n",
    "    return metrics, y_true, y_pred, y_prob\n",
    "\n",
    "# --- Create test loader if you haven't already ---\n",
    "if 'test_df' in locals() and len(test_df) > 0:\n",
    "    test_ds = MultiLabelImageDataset(test_df, transform=val_tf)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS, \n",
    "        pin_memory=False, \n",
    "        collate_fn=collate_fn_skip_error\n",
    "    )\n",
    "    print(f\"Created test loader with {len(test_ds)} images\")\n",
    "else:\n",
    "    print(\"No test set available. Using validation set for evaluation.\")\n",
    "    test_loader = val_loader\n",
    "\n",
    "# --- Run evaluation ---\n",
    "print(\"\\nEvaluating model on test set...\")\n",
    "metrics, y_true, y_pred, y_prob = evaluate_model(model, test_loader)\n",
    "\n",
    "# --- Print metrics ---\n",
    "print(\"\\n===== Model Evaluation Results =====\")\n",
    "print(f\"Hamming Loss: {metrics['hamming_loss']:.4f} (lower is better)\")\n",
    "print(f\"Exact Match Accuracy: {metrics['exact_match']:.4f}\")\n",
    "print(f\"Micro-F1 Score: {metrics['f1_micro']:.4f}\")\n",
    "print(f\"Macro-F1 Score: {metrics['f1_macro']:.4f}\")\n",
    "print(f\"Micro-Precision: {metrics['precision_micro']:.4f}\")\n",
    "print(f\"Micro-Recall: {metrics['recall_micro']:.4f}\")\n",
    "\n",
    "# --- Plot confusion matrix for each emotion ---\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, emotion in enumerate(emotions):\n",
    "    plt.subplot(3, 3, i+1 if i < 8 else 9)\n",
    "    cm = confusion_matrix(y_true[:, i], y_pred[:, i])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Not '+emotion, emotion], \n",
    "                yticklabels=['Not '+emotion, emotion])\n",
    "    plt.title(f\"{emotion}: F1={metrics[f'f1_{emotion}']:.2f}, AUC={metrics.get(f'auc_{emotion}', 'N/A')}\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'emotion_confusion_matrices_{CURRENT_TIME}.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot the distribution of predicted emotions ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "emotion_counts_true = y_true.sum(axis=0)\n",
    "emotion_counts_pred = y_pred.sum(axis=0)\n",
    "indices = np.arange(len(emotions))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(indices - width/2, emotion_counts_true, width, label='True')\n",
    "plt.bar(indices + width/2, emotion_counts_pred, width, label='Predicted')\n",
    "plt.xticks(indices, emotions, rotation=45, ha='right')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of True vs Predicted Emotions')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'emotion_distribution_{CURRENT_TIME}.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- Save evaluation results ---\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': list(metrics.keys()),\n",
    "    'Value': list(metrics.values())\n",
    "})\n",
    "results_df.to_csv(f'evaluation_results_{CURRENT_TIME}.csv', index=False)\n",
    "print(f\"Evaluation results saved to evaluation_results_{CURRENT_TIME}.csv\")\n",
    "\n",
    "# --- Save predictions for further analysis ---\n",
    "pred_df = pd.DataFrame(y_prob, columns=emotions)\n",
    "if 'test_df' in locals() and len(test_df) > 0:\n",
    "    # Add image paths\n",
    "    pred_df['image_file'] = test_df['image_file'].values\n",
    "    \n",
    "pred_df.to_csv(f'predictions_{CURRENT_TIME}.csv', index=False)\n",
    "print(f\"Predictions saved to predictions_{CURRENT_TIME}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83bacb-7347-4133-b721-ea4a282ea359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
