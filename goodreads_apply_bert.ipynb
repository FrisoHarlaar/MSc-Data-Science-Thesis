{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515eace7-1e4b-42cf-9562-521f9bd2633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERT Emotion Analysis Configuration initialized\n",
      "🤖 Device: cuda\n",
      "📦 Batch size: 16\n",
      "📏 Max length: 512\n",
      "✅ Artemis emotion modules imported successfully\n",
      "📊 Emotion classes: ['amusement', 'awe', 'contentment', 'excitement', 'anger', 'disgust', 'fear', 'sadness', 'something else']\n",
      "📊 Number of labels: 9\n",
      "📋 BERT EMOTION ANALYSIS CONFIGURATION\n",
      "--------------------------------------------------\n",
      "📁 Artemis path: data/artemis/artemis\n",
      "🤖 BERT model directory: data/artemis/artemis/predictions/bert_based/best_model\n",
      "📊 Data file: preprocessed_books_2025_04_20.parquet\n",
      "💾 Results directory: goodreads_bert_emotion_results\n",
      "🖥️ Device: cuda\n",
      "📦 Batch size: 16\n",
      "📏 Max sequence length: 512\n",
      "🎯 Expected emotion classes: 9\n",
      "--------------------------------------------------\n",
      "✅ Model directory found with files: ['model.safetensors', 'config.json', 'tokenizer_config.json', 'vocab.txt', 'special_tokens_map.json', 'tokenizer.json']\n",
      "Ready to run! Execute: results = analyze_goodreads_bert_emotions()\n",
      "🤖 BERT-based Goodreads Book Description Emotion Analysis\n",
      "📚 Processing English books with descriptions only\n",
      "🔄 Using fine-tuned BERT model from ArtEmis training\n",
      "------------------------------------------------------------\n",
      "🤖 Loading fine-tuned BERT model from data/artemis/artemis/predictions/bert_based/best_model\n",
      "📁 Files in model directory: ['model.safetensors', 'config.json', 'tokenizer_config.json', 'vocab.txt', 'special_tokens_map.json', 'tokenizer.json']\n",
      "✅ BERT model loaded successfully\n",
      "📊 Model expects 9 emotion classes\n",
      "🎯 Expected emotions: 9\n",
      "🚀 Starting BERT-based Goodreads emotion analysis...\n",
      "🤖 Using model from: data/artemis/artemis/predictions/bert_based/best_model\n",
      "📂 Loading Goodreads data...\n",
      "📊 Loaded 931,229 total books\n",
      "🔍 Filtering for English books from 931,229 total books\n",
      "✅ Filtered to 687,029 English books\n",
      "📉 Dropped 244,200 non-English books (26.2%)\n",
      "🔍 Filtering books with descriptions from 687,029 books\n",
      "📝 Using description field: 'description'\n",
      "✅ Found 686,946 books with meaningful descriptions\n",
      "📉 Dropped 83 books without descriptions (0.0%)\n",
      "📂 Loaded BERT checkpoint from 2025-06-04T15:21:20.619587\n",
      "📊 Previous progress: 380,000 results, 0 failed\n",
      "🔄 Resuming from checkpoint: 380,000 books already processed\n",
      "📋 Processing 306,946 remaining English books with descriptions\n",
      "🔄 Processing 62 batches of up to 5,000 books each\n",
      "🎯 Using BERT batch size of 16 for predictions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7711188518c46f0961cdb35b45af34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing BERT batches:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Progress: 56.0% (385,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 12827.0 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 385,000 results, 0 failed\n",
      "📊 Progress: 56.8% (390,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 5830.6 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 390,000 results, 0 failed\n",
      "📊 Progress: 57.5% (395,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 3873.0 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 395,000 results, 0 failed\n",
      "📊 Progress: 58.2% (400,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 2900.9 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 400,000 results, 0 failed\n",
      "📊 Progress: 59.0% (405,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 2298.5 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 405,000 results, 0 failed\n",
      "📊 Progress: 59.7% (410,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 1896.5 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 410,000 results, 0 failed\n",
      "📊 Progress: 60.4% (415,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 1628.1 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 415,000 results, 0 failed\n",
      "📊 Progress: 61.1% (420,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 1436.0 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 420,000 results, 0 failed\n",
      "📊 Progress: 61.9% (425,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 1283.8 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 425,000 results, 0 failed\n",
      "📊 Progress: 62.6% (430,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 1159.6 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 430,000 results, 0 failed\n",
      "📊 Progress: 63.3% (435,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 1057.1 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 435,000 results, 0 failed\n",
      "📊 Progress: 64.1% (440,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 976.4 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 440,000 results, 0 failed\n",
      "📊 Progress: 64.8% (445,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 908.9 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 445,000 results, 0 failed\n",
      "📊 Progress: 65.5% (450,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 848.5 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 450,000 results, 0 failed\n",
      "📊 Progress: 66.2% (455,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 795.9 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 455,000 results, 0 failed\n",
      "📊 Progress: 67.0% (460,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 749.4 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 460,000 results, 0 failed\n",
      "📊 Progress: 67.7% (465,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 710.3 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 465,000 results, 0 failed\n",
      "📊 Progress: 68.4% (470,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 676.7 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 470,000 results, 0 failed\n",
      "📊 Progress: 69.1% (475,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 642.6 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 475,000 results, 0 failed\n",
      "📊 Progress: 69.9% (480,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 614.6 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 480,000 results, 0 failed\n",
      "📊 Progress: 70.6% (485,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 587.2 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 485,000 results, 0 failed\n",
      "📊 Progress: 71.3% (490,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 563.8 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 490,000 results, 0 failed\n",
      "📊 Progress: 72.1% (495,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 542.2 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 495,000 results, 0 failed\n",
      "📊 Progress: 72.8% (500,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 522.8 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 500,000 results, 0 failed\n",
      "📊 Progress: 73.5% (505,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 504.9 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 505,000 results, 0 failed\n",
      "📊 Progress: 74.2% (510,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 487.5 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 510,000 results, 0 failed\n",
      "📊 Progress: 75.0% (515,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 472.2 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 515,000 results, 0 failed\n",
      "📊 Progress: 75.7% (520,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 458.3 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 520,000 results, 0 failed\n",
      "📊 Progress: 76.4% (525,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 444.5 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 525,000 results, 0 failed\n",
      "📊 Progress: 77.2% (530,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 432.3 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 530,000 results, 0 failed\n",
      "📊 Progress: 77.9% (535,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 421.0 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 535,000 results, 0 failed\n",
      "📊 Progress: 78.6% (540,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 410.0 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 540,000 results, 0 failed\n",
      "📊 Progress: 79.3% (545,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 399.6 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 545,000 results, 0 failed\n",
      "📊 Progress: 80.1% (550,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 390.4 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 550,000 results, 0 failed\n",
      "📊 Progress: 80.8% (555,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 381.3 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 555,000 results, 0 failed\n",
      "📊 Progress: 81.5% (560,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 372.5 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 560,000 results, 0 failed\n",
      "📊 Progress: 82.2% (565,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 364.2 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 565,000 results, 0 failed\n",
      "📊 Progress: 83.0% (570,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 356.6 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 570,000 results, 0 failed\n",
      "📊 Progress: 83.7% (575,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 349.6 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 575,000 results, 0 failed\n",
      "📊 Progress: 84.4% (580,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 343.0 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 580,000 results, 0 failed\n",
      "📊 Progress: 85.2% (585,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 336.6 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 585,000 results, 0 failed\n",
      "📊 Progress: 85.9% (590,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 330.2 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 590,000 results, 0 failed\n",
      "📊 Progress: 86.6% (595,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 324.3 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 595,000 results, 0 failed\n",
      "📊 Progress: 87.3% (600,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 318.5 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 600,000 results, 0 failed\n",
      "📊 Progress: 88.1% (605,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 313.2 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 605,000 results, 0 failed\n",
      "📊 Progress: 88.8% (610,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 308.1 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 610,000 results, 0 failed\n",
      "📊 Progress: 89.5% (615,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 303.1 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 615,000 results, 0 failed\n",
      "📊 Progress: 90.3% (620,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 298.6 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 620,000 results, 0 failed\n",
      "📊 Progress: 91.0% (625,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 294.0 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 625,000 results, 0 failed\n",
      "📊 Progress: 91.7% (630,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 289.5 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 630,000 results, 0 failed\n",
      "📊 Progress: 92.4% (635,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 285.5 books/sec | ⏱️ ETA: 0.1h\n",
      "💾 BERT Checkpoint saved: 635,000 results, 0 failed\n",
      "📊 Progress: 93.2% (640,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 281.6 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 640,000 results, 0 failed\n",
      "📊 Progress: 93.9% (645,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 277.6 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 645,000 results, 0 failed\n",
      "📊 Progress: 94.6% (650,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 273.7 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 650,000 results, 0 failed\n",
      "📊 Progress: 95.3% (655,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 270.2 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 655,000 results, 0 failed\n",
      "📊 Progress: 96.1% (660,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 266.8 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 660,000 results, 0 failed\n",
      "📊 Progress: 96.8% (665,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 263.3 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 665,000 results, 0 failed\n",
      "📊 Progress: 97.5% (670,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 259.9 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 670,000 results, 0 failed\n",
      "📊 Progress: 98.3% (675,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 256.7 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 675,000 results, 0 failed\n",
      "📊 Progress: 99.0% (680,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 253.8 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 680,000 results, 0 failed\n",
      "📊 Progress: 99.7% (685,000/686,946) | ✅ Success: 100.0% | 🚀 Speed: 250.8 books/sec | ⏱️ ETA: 0.0h\n",
      "💾 BERT Checkpoint saved: 685,000 results, 0 failed\n",
      "💾 BERT Checkpoint saved: 686,946 results, 0 failed\n",
      "\n",
      "============================================================\n",
      "🎉 BERT EMOTION ANALYSIS COMPLETED!\n",
      "============================================================\n",
      "📊 Total processed: 686,946\n",
      "✅ Successful: 686,946 (100.00%)\n",
      "❌ Failed: 0\n",
      "💾 Results saved to: goodreads_bert_emotion_results/goodreads_bert_emotion_predictions_20250604_160958.parquet\n",
      "============================================================\n",
      "\n",
      "📈 Emotion Distribution:\n",
      "  sadness: 192,494 (28.0%)\n",
      "  amusement: 110,417 (16.1%)\n",
      "  excitement: 100,331 (14.6%)\n",
      "  fear: 94,141 (13.7%)\n",
      "  awe: 89,756 (13.1%)\n",
      "  anger: 35,642 (5.2%)\n",
      "  contentment: 26,742 (3.9%)\n",
      "  something else: 19,989 (2.9%)\n",
      "  disgust: 17,434 (2.5%)\n",
      "\n",
      "📊 Emotion Categories (Positive/Negative/Else):\n",
      "  1: 339,711 (49.5%)\n",
      "  0: 327,246 (47.6%)\n",
      "  2: 19,989 (2.9%)\n",
      "\n",
      "🎯 Confidence Statistics:\n",
      "  Mean confidence: 0.481\n",
      "  Median confidence: 0.441\n",
      "  High confidence (>0.8): 54,237 (7.9%)\n"
     ]
    }
   ],
   "source": [
    "# Goodreads Book Description Emotion Analysis using Fine-tuned BERT\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "from queue import Queue\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import unicodedata\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Enable nested async loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('goodreads_bert_emotion_analysis.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration matching your BERT training script\n",
    "class BERTEmotionConfig:\n",
    "    # Paths - UPDATE THESE TO MATCH YOUR SETUP\n",
    "    ARTEMIS_PATH = r'data/artemis/artemis'  # Your artemis path\n",
    "    BERT_MODEL_DIR = r'data/artemis/artemis/predictions/bert_based/best_model'  # Your trained BERT model path\n",
    "    DATA_FILE = 'preprocessed_books_2025_04_20.parquet'  # Your Goodreads data\n",
    "    RESULTS_DIR = 'goodreads_bert_emotion_results'\n",
    "    \n",
    "    # BERT configuration (matching your training script exactly)\n",
    "    MAX_LENGTH = 512  # Same as your training\n",
    "    BATCH_SIZE = 16   # Same as your training script\n",
    "    MODEL_NAME = 'google-bert/bert-base-uncased'  # Same base model\n",
    "    \n",
    "    # Processing configuration\n",
    "    PROCESSING_BATCH_SIZE = 5000  # Smaller batches for BERT\n",
    "    CHECKPOINT_FREQUENCY = 2500\n",
    "    \n",
    "    # Progress tracking\n",
    "    PROGRESS_UPDATE_FREQUENCY = 500\n",
    "    DETAILED_LOG_FREQUENCY = 1000\n",
    "    \n",
    "    # English language filtering\n",
    "    ENGLISH_CODES = {\n",
    "        'en', 'eng', 'en-us', 'en-gb', 'en-ca', 'en-au', 'en-nz', 'en-za', \n",
    "        'en-in', 'english', 'en_us', 'en_gb', 'en_ca', 'en_au'\n",
    "    }\n",
    "    \n",
    "    # GPU configuration\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def __init__(self):\n",
    "        os.makedirs(self.RESULTS_DIR, exist_ok=True)\n",
    "        os.makedirs(f\"{self.RESULTS_DIR}/checkpoints\", exist_ok=True)\n",
    "        print(f\"✅ BERT Emotion Analysis Configuration initialized\")\n",
    "        print(f\"🤖 Device: {self.DEVICE}\")\n",
    "        print(f\"📦 Batch size: {self.BATCH_SIZE}\")\n",
    "        print(f\"📏 Max length: {self.MAX_LENGTH}\")\n",
    "\n",
    "config = BERTEmotionConfig()\n",
    "\n",
    "# Add artemis to path\n",
    "# if config.ARTEMIS_PATH not in sys.path:\n",
    "#     sys.path.append(config.ARTEMIS_PATH)\n",
    "\n",
    "# Import artemis modules (matching your training script)\n",
    "try:\n",
    "    from artemis.emotions import ARTEMIS_EMOTIONS, IDX_TO_EMOTION, positive_negative_else\n",
    "    from artemis.in_out.basics import create_dir\n",
    "    print(\"✅ Artemis emotion modules imported successfully\")\n",
    "    print(f\"📊 Emotion classes: {ARTEMIS_EMOTIONS}\")\n",
    "    print(f\"📊 Number of labels: {len(ARTEMIS_EMOTIONS)}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing artemis modules: {e}\")\n",
    "    print(\"Please check your ARTEMIS_PATH in the config above\")\n",
    "\n",
    "def preprocess_text_artemis_style(text):\n",
    "    \"\"\"\n",
    "    Preprocess text following ArtEmis conventions\n",
    "    This should match the preprocessing used in utterance_spelled\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and normalize unicode (like ArtEmis does)\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Basic cleaning while preserving meaningful content\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove very long sequences that might cause issues\n",
    "    # But keep the text mostly intact since BERT can handle various formats\n",
    "    if len(text) > 10000:  # Very long descriptions\n",
    "        text = text[:10000] + \"...\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "def filter_english_books(books_df):\n",
    "    \"\"\"Filter books to include only English language books\"\"\"\n",
    "    print(f\"🔍 Filtering for English books from {len(books_df):,} total books\")\n",
    "    \n",
    "    def is_english(lang_code):\n",
    "        if pd.isna(lang_code) or lang_code == '':\n",
    "            return False\n",
    "        lang_code_clean = str(lang_code).lower().strip()\n",
    "        return lang_code_clean in config.ENGLISH_CODES\n",
    "    english_mask = books_df['language_code'].astype(str).apply(is_english)\n",
    "    english_books = books_df[english_mask].copy()\n",
    "    \n",
    "    dropped_count = len(books_df) - len(english_books)\n",
    "    print(f\"✅ Filtered to {len(english_books):,} English books\")\n",
    "    print(f\"📉 Dropped {dropped_count:,} non-English books ({dropped_count/len(books_df)*100:.1f}%)\")\n",
    "    \n",
    "    return english_books\n",
    "\n",
    "def filter_books_with_descriptions(books_df):\n",
    "    \"\"\"Filter books that have descriptions\"\"\"\n",
    "    print(f\"🔍 Filtering books with descriptions from {len(books_df):,} books\")\n",
    "    \n",
    "    # Check for description fields (common field names in Goodreads data)\n",
    "    description_fields = ['description', 'book_description', 'summary', 'plot', 'desc']\n",
    "    description_field = None\n",
    "    \n",
    "    for field in description_fields:\n",
    "        if field in books_df.columns:\n",
    "            description_field = field\n",
    "            break\n",
    "    \n",
    "    if description_field is None:\n",
    "        print(\"❌ No description field found in the dataset\")\n",
    "        print(f\"Available columns: {list(books_df.columns)}\")\n",
    "        # Return empty dataframe if no description field\n",
    "        return books_df.iloc[:0].copy(), None\n",
    "    \n",
    "    print(f\"📝 Using description field: '{description_field}'\")\n",
    "    \n",
    "    # Filter books with non-empty descriptions\n",
    "    has_description = (\n",
    "        books_df[description_field].notna() & \n",
    "        (books_df[description_field].astype(str).str.strip() != '') &\n",
    "        (books_df[description_field].astype(str).str.len() > 10)  # At least 10 characters\n",
    "    )\n",
    "    \n",
    "    books_with_desc = books_df[has_description].copy()\n",
    "    \n",
    "    dropped_count = len(books_df) - len(books_with_desc)\n",
    "    print(f\"✅ Found {len(books_with_desc):,} books with meaningful descriptions\")\n",
    "    print(f\"📉 Dropped {dropped_count:,} books without descriptions ({dropped_count/len(books_df)*100:.1f}%)\")\n",
    "    \n",
    "    return books_with_desc, description_field\n",
    "\n",
    "class BERTProgressTracker:\n",
    "    \"\"\"Progress tracker for BERT emotion analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, total_books):\n",
    "        self.total_books = total_books\n",
    "        self.processed_books = 0\n",
    "        self.successful_books = 0\n",
    "        self.failed_books = 0\n",
    "        self.start_time = time.time()\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # Performance metrics\n",
    "        self.books_per_second = 0\n",
    "        \n",
    "    def update(self, successful=0, failed=0):\n",
    "        with self.lock:\n",
    "            self.successful_books += successful\n",
    "            self.failed_books += failed\n",
    "            self.processed_books = self.successful_books + self.failed_books\n",
    "            \n",
    "            # Calculate performance\n",
    "            elapsed = time.time() - self.start_time\n",
    "            if elapsed > 0:\n",
    "                self.books_per_second = self.processed_books / elapsed\n",
    "    \n",
    "    def should_log(self, frequency):\n",
    "        return self.processed_books % frequency == 0\n",
    "    \n",
    "    def get_status_message(self):\n",
    "        with self.lock:\n",
    "            progress_pct = (self.processed_books / self.total_books) * 100\n",
    "            success_rate = (self.successful_books / max(1, self.processed_books)) * 100\n",
    "            \n",
    "            remaining = self.total_books - self.processed_books\n",
    "            eta_seconds = remaining / max(0.1, self.books_per_second)\n",
    "            eta_hours = eta_seconds / 3600\n",
    "            \n",
    "            elapsed_hours = (time.time() - self.start_time) / 3600\n",
    "            \n",
    "            msg = f\"📊 Progress: {progress_pct:.1f}% \"\n",
    "            msg += f\"({self.processed_books:,}/{self.total_books:,}) | \"\n",
    "            msg += f\"✅ Success: {success_rate:.1f}% | \"\n",
    "            msg += f\"🚀 Speed: {self.books_per_second:.1f} books/sec | \"\n",
    "            msg += f\"⏱️ ETA: {eta_hours:.1f}h\"\n",
    "            \n",
    "            return msg\n",
    "\n",
    "class BERTCheckpointManager:\n",
    "    \"\"\"Checkpoint manager for BERT processing\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, 'bert_progress_checkpoint.json')\n",
    "        self.results_file = os.path.join(checkpoint_dir, 'bert_partial_results.pkl')\n",
    "        \n",
    "    def save_checkpoint(self, processed_indices, results, failed_books, progress_tracker):\n",
    "        \"\"\"Save checkpoint\"\"\"\n",
    "        checkpoint_data = {\n",
    "            'processed_indices': list(processed_indices),\n",
    "            'num_results': len(results),\n",
    "            'num_failed': len(failed_books),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'books_per_second': progress_tracker.books_per_second,\n",
    "            'success_rate': progress_tracker.successful_books / max(1, progress_tracker.processed_books),\n",
    "            'bert_version': True\n",
    "        }\n",
    "        \n",
    "        # Save checkpoint metadata\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "        \n",
    "        # Save actual results\n",
    "        with open(self.results_file, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'results': results,\n",
    "                'failed_books': failed_books\n",
    "            }, f)\n",
    "        \n",
    "        print(f\"💾 BERT Checkpoint saved: {len(results):,} results, {len(failed_books):,} failed\")\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load checkpoint\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file) and os.path.exists(self.results_file):\n",
    "            try:\n",
    "                # Load metadata\n",
    "                with open(self.checkpoint_file, 'r') as f:\n",
    "                    checkpoint_data = json.load(f)\n",
    "                \n",
    "                # Load results\n",
    "                with open(self.results_file, 'rb') as f:\n",
    "                    saved_data = pickle.load(f)\n",
    "                \n",
    "                print(f\"📂 Loaded BERT checkpoint from {checkpoint_data['timestamp']}\")\n",
    "                print(f\"📊 Previous progress: {checkpoint_data['num_results']:,} results, {checkpoint_data['num_failed']:,} failed\")\n",
    "                \n",
    "                return (\n",
    "                    set(checkpoint_data['processed_indices']),\n",
    "                    saved_data['results'],\n",
    "                    saved_data['failed_books']\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading BERT checkpoint: {e}\")\n",
    "                return set(), [], []\n",
    "        \n",
    "        return set(), [], []\n",
    "\n",
    "class BERTEmotionPredictor:\n",
    "    \"\"\"BERT-based emotion predictor for Goodreads descriptions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.checkpoint_manager = BERTCheckpointManager(f\"{config.RESULTS_DIR}/checkpoints\")\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.progress_tracker = None\n",
    "        self.load_model()\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the fine-tuned BERT model and tokenizer\"\"\"\n",
    "        try:\n",
    "            print(f\"🤖 Loading fine-tuned BERT model from {config.BERT_MODEL_DIR}\")\n",
    "            \n",
    "            # Check if model directory exists\n",
    "            if not os.path.exists(config.BERT_MODEL_DIR):\n",
    "                print(f\"❌ Model directory not found: {config.BERT_MODEL_DIR}\")\n",
    "                print(\"Please ensure you have trained the model using utterance_to_emotion_with_transformer_my_try.py\")\n",
    "                print(\"And that the paths match your artemis directory structure\")\n",
    "                raise FileNotFoundError(f\"Model directory not found: {config.BERT_MODEL_DIR}\")\n",
    "            \n",
    "            # List files in model directory for debugging\n",
    "            model_files = os.listdir(config.BERT_MODEL_DIR)\n",
    "            print(f\"📁 Files in model directory: {model_files}\")\n",
    "            \n",
    "            # Load tokenizer and model\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(config.BERT_MODEL_DIR)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(config.BERT_MODEL_DIR)\n",
    "            \n",
    "            # Move to device\n",
    "            self.model.to(config.DEVICE)\n",
    "            self.model.eval()\n",
    "            \n",
    "            print(f\"✅ BERT model loaded successfully\")\n",
    "            print(f\"📊 Model expects {self.model.config.num_labels} emotion classes\")\n",
    "            print(f\"🎯 Expected emotions: {len(ARTEMIS_EMOTIONS)}\")\n",
    "            \n",
    "            # Verify model configuration\n",
    "            if self.model.config.num_labels != len(ARTEMIS_EMOTIONS):\n",
    "                print(f\"⚠️ Warning: Model has {self.model.config.num_labels} labels, but ArtEmis has {len(ARTEMIS_EMOTIONS)} emotions\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading BERT model: {e}\")\n",
    "            print(\"Please ensure you have:\")\n",
    "            print(\"1. Trained the model using utterance_to_emotion_with_transformer_my_try.py\")\n",
    "            print(\"2. Set do_training=True in that script\")\n",
    "            print(\"3. Updated the paths to match your directory structure\")\n",
    "            raise\n",
    "    \n",
    "    def predict_emotions_batch(self, texts):\n",
    "        \"\"\"Predict emotions for a batch of texts\"\"\"\n",
    "        try:\n",
    "            # Tokenize texts (same parameters as training)\n",
    "            encodings = self.tokenizer(\n",
    "                texts,\n",
    "                truncation=True,\n",
    "                padding='max_length',  # Same as training\n",
    "                max_length=config.MAX_LENGTH,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            input_ids = encodings['input_ids'].to(config.DEVICE)\n",
    "            attention_mask = encodings['attention_mask'].to(config.DEVICE)\n",
    "            \n",
    "            # Predict\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Convert to probabilities\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                predictions = torch.argmax(probabilities, dim=-1)\n",
    "            \n",
    "            return predictions.cpu().numpy(), probabilities.cpu().numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in BERT prediction: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def process_batch(self, batch_df, description_field):\n",
    "        \"\"\"Process a batch of books\"\"\"\n",
    "        batch_results = []\n",
    "        batch_failed = []\n",
    "        \n",
    "        # Extract and preprocess descriptions\n",
    "        descriptions = []\n",
    "        book_data = []\n",
    "        \n",
    "        for idx, row in batch_df.iterrows():\n",
    "            description = row.get(description_field, '')\n",
    "            \n",
    "            if pd.isna(description) or str(description).strip() == '':\n",
    "                batch_failed.append({\n",
    "                    'book_id': row.get('book_id', idx),\n",
    "                    'reason': 'no_description'\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Preprocess description (minimal preprocessing to match training)\n",
    "            processed_desc = preprocess_text_artemis_style(description)\n",
    "            \n",
    "            if len(processed_desc.strip()) < 5:  # Very short descriptions\n",
    "                batch_failed.append({\n",
    "                    'book_id': row.get('book_id', idx),\n",
    "                    'reason': 'description_too_short'\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            descriptions.append(processed_desc)\n",
    "            book_data.append((idx, row))\n",
    "        \n",
    "        if not descriptions:\n",
    "            return batch_results, batch_failed\n",
    "        \n",
    "        # Process in smaller sub-batches for BERT (same batch size as training)\n",
    "        for i in range(0, len(descriptions), config.BATCH_SIZE):\n",
    "            sub_descriptions = descriptions[i:i+config.BATCH_SIZE]\n",
    "            sub_book_data = book_data[i:i+config.BATCH_SIZE]\n",
    "            \n",
    "            # Predict emotions\n",
    "            predictions, probabilities = self.predict_emotions_batch(sub_descriptions)\n",
    "            \n",
    "            if predictions is None:\n",
    "                # All failed in this sub-batch\n",
    "                for j, (idx, row) in enumerate(sub_book_data):\n",
    "                    batch_failed.append({\n",
    "                        'book_id': row.get('book_id', idx),\n",
    "                        'reason': 'prediction_failed'\n",
    "                    })\n",
    "                continue\n",
    "            \n",
    "            # Process results\n",
    "            for j, (idx, row) in enumerate(sub_book_data):\n",
    "                pred_idx = predictions[j]\n",
    "                emotion_probs = probabilities[j]\n",
    "                \n",
    "                # Get emotion label using IDX_TO_EMOTION (same as training)\n",
    "                predicted_emotion = IDX_TO_EMOTION[pred_idx]\n",
    "                confidence = emotion_probs[pred_idx]\n",
    "                \n",
    "                # Get positive/negative/else classification (same as training)\n",
    "                emotion_pne = positive_negative_else(predicted_emotion)\n",
    "                \n",
    "                result = {\n",
    "                    'book_id': row.get('book_id', idx),\n",
    "                    'title': row.get('title', ''),\n",
    "                    'authors': row.get('authors', ''),\n",
    "                    'average_rating': row.get('average_rating'),\n",
    "                    'ratings_count': row.get('ratings_count'),\n",
    "                    'publication_year': row.get('publication_year'),\n",
    "                    'language_code': row.get('language_code'),\n",
    "                    'popular_shelves': row.get('popular_shelves'),\n",
    "                    'description_preview': sub_descriptions[j][:200] + '...' if len(sub_descriptions[j]) > 200 else sub_descriptions[j],\n",
    "                    'description_length': len(sub_descriptions[j]),\n",
    "                    'predicted_emotion': predicted_emotion,\n",
    "                    'emotion_category': emotion_pne,\n",
    "                    'confidence': float(confidence),\n",
    "                    'emotion_probs': emotion_probs.tolist()\n",
    "                }\n",
    "                \n",
    "                # Add individual emotion probabilities (same as training order)\n",
    "                for k, emotion in enumerate(ARTEMIS_EMOTIONS):\n",
    "                    result[f'prob_{emotion.replace(\" \", \"_\")}'] = float(emotion_probs[k])\n",
    "                \n",
    "                batch_results.append(result)\n",
    "        \n",
    "        return batch_results, batch_failed\n",
    "    \n",
    "    def process_all_books(self, resume_from_checkpoint=True):\n",
    "        \"\"\"Main processing function\"\"\"\n",
    "        \n",
    "        print(\"🚀 Starting BERT-based Goodreads emotion analysis...\")\n",
    "        print(f\"🤖 Using model from: {config.BERT_MODEL_DIR}\")\n",
    "        \n",
    "        # Load and filter data\n",
    "        print(\"📂 Loading Goodreads data...\")\n",
    "        try:\n",
    "            books_df = pd.read_parquet(config.DATA_FILE)\n",
    "            print(f\"📊 Loaded {len(books_df):,} total books\")\n",
    "            \n",
    "            # Filter for English books\n",
    "            books_df = filter_english_books(books_df)\n",
    "            \n",
    "            # Filter for books with descriptions\n",
    "            books_df, description_field = filter_books_with_descriptions(books_df)\n",
    "            \n",
    "            if description_field is None:\n",
    "                print(\"❌ No description field found. Cannot proceed.\")\n",
    "                return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading data: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Initialize progress tracking\n",
    "        self.progress_tracker = BERTProgressTracker(len(books_df))\n",
    "        \n",
    "        # Load previous progress if resuming\n",
    "        processed_indices = set()\n",
    "        all_results = []\n",
    "        all_failed = []\n",
    "        \n",
    "        if resume_from_checkpoint:\n",
    "            processed_indices, all_results, all_failed = self.checkpoint_manager.load_checkpoint()\n",
    "            if processed_indices:\n",
    "                print(f\"🔄 Resuming from checkpoint: {len(processed_indices):,} books already processed\")\n",
    "                # Update progress tracker\n",
    "                self.progress_tracker.update(\n",
    "                    successful=len(all_results),\n",
    "                    failed=len(all_failed)\n",
    "                )\n",
    "        \n",
    "        # Filter out already processed books\n",
    "        remaining_books = books_df[~books_df.index.isin(processed_indices)].copy()\n",
    "        print(f\"📋 Processing {len(remaining_books):,} remaining English books with descriptions\")\n",
    "        \n",
    "        if len(remaining_books) == 0:\n",
    "            print(\"✅ All books already processed!\")\n",
    "            return pd.DataFrame(all_results)\n",
    "        \n",
    "        try:\n",
    "            # Process in batches\n",
    "            total_batches = (len(remaining_books) + config.PROCESSING_BATCH_SIZE - 1) // config.PROCESSING_BATCH_SIZE\n",
    "            \n",
    "            print(f\"🔄 Processing {total_batches} batches of up to {config.PROCESSING_BATCH_SIZE:,} books each\")\n",
    "            print(f\"🎯 Using BERT batch size of {config.BATCH_SIZE} for predictions\")\n",
    "            \n",
    "            # Create tqdm progress bar\n",
    "            batch_progress = tqdm(range(total_batches), desc=\"Processing BERT batches\")\n",
    "            \n",
    "            for batch_idx in batch_progress:\n",
    "                start_idx = batch_idx * config.PROCESSING_BATCH_SIZE\n",
    "                end_idx = min(start_idx + config.PROCESSING_BATCH_SIZE, len(remaining_books))\n",
    "                batch_df = remaining_books.iloc[start_idx:end_idx]\n",
    "                \n",
    "                # Process this batch\n",
    "                batch_results, batch_failed = self.process_batch(batch_df, description_field)\n",
    "                \n",
    "                # Update results\n",
    "                all_results.extend(batch_results)\n",
    "                all_failed.extend(batch_failed)\n",
    "                \n",
    "                # Update processed indices\n",
    "                for idx in batch_df.index:\n",
    "                    processed_indices.add(idx)\n",
    "                \n",
    "                # Update progress\n",
    "                self.progress_tracker.update(\n",
    "                    successful=len(batch_results),\n",
    "                    failed=len(batch_failed)\n",
    "                )\n",
    "                \n",
    "                # Update progress bar\n",
    "                success_rate = self.progress_tracker.successful_books / max(1, self.progress_tracker.processed_books) * 100\n",
    "                batch_progress.set_postfix({\n",
    "                    'Success Rate': f\"{success_rate:.1f}%\",\n",
    "                    'Speed': f\"{self.progress_tracker.books_per_second:.1f} books/sec\"\n",
    "                })\n",
    "                \n",
    "                # Log progress\n",
    "                if self.progress_tracker.should_log(config.PROGRESS_UPDATE_FREQUENCY):\n",
    "                    print(self.progress_tracker.get_status_message())\n",
    "                \n",
    "                # Save checkpoint\n",
    "                checkpoint_interval = max(1, config.CHECKPOINT_FREQUENCY // config.PROCESSING_BATCH_SIZE)\n",
    "                if (batch_idx + 1) % checkpoint_interval == 0:\n",
    "                    self.checkpoint_manager.save_checkpoint(\n",
    "                        processed_indices, all_results, all_failed, self.progress_tracker\n",
    "                    )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during processing: {e}\")\n",
    "            # Save emergency checkpoint\n",
    "            self.checkpoint_manager.save_checkpoint(\n",
    "                processed_indices, all_results, all_failed, self.progress_tracker\n",
    "            )\n",
    "            return None\n",
    "        \n",
    "        # Save final results\n",
    "        return self.save_final_results(all_results, all_failed)\n",
    "    \n",
    "    def save_final_results(self, results, failed):\n",
    "        \"\"\"Save final results\"\"\"\n",
    "        results_df = pd.DataFrame(results)\n",
    "        failed_df = pd.DataFrame(failed)\n",
    "        \n",
    "        # Save files\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results_file = f\"{config.RESULTS_DIR}/goodreads_bert_emotion_predictions_{timestamp}.parquet\"\n",
    "        failed_file = f\"{config.RESULTS_DIR}/goodreads_bert_failed_books_{timestamp}.csv\"\n",
    "        \n",
    "        results_df.to_parquet(results_file, index=False)\n",
    "        failed_df.to_csv(failed_file, index=False)\n",
    "        \n",
    "        # Clear checkpoint files\n",
    "        try:\n",
    "            if os.path.exists(self.checkpoint_manager.checkpoint_file):\n",
    "                os.remove(self.checkpoint_manager.checkpoint_file)\n",
    "            if os.path.exists(self.checkpoint_manager.results_file):\n",
    "                os.remove(self.checkpoint_manager.results_file)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Final statistics\n",
    "        total_processed = len(results) + len(failed)\n",
    "        success_rate = len(results) / total_processed * 100 if total_processed > 0 else 0\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🎉 BERT EMOTION ANALYSIS COMPLETED!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"📊 Total processed: {total_processed:,}\")\n",
    "        print(f\"✅ Successful: {len(results):,} ({success_rate:.2f}%)\")\n",
    "        print(f\"❌ Failed: {len(failed):,}\")\n",
    "        print(f\"💾 Results saved to: {results_file}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Emotion analysis\n",
    "        if len(results) > 0:\n",
    "            print(\"\\n📈 Emotion Distribution:\")\n",
    "            emotion_counts = results_df['predicted_emotion'].value_counts()\n",
    "            for emotion, count in emotion_counts.items():\n",
    "                pct = count / len(results_df) * 100\n",
    "                print(f\"  {emotion}: {count:,} ({pct:.1f}%)\")\n",
    "            \n",
    "            print(\"\\n📊 Emotion Categories (Positive/Negative/Else):\")\n",
    "            category_counts = results_df['emotion_category'].value_counts()\n",
    "            for category, count in category_counts.items():\n",
    "                pct = count / len(results_df) * 100\n",
    "                print(f\"  {category}: {count:,} ({pct:.1f}%)\")\n",
    "            \n",
    "            print(\"\\n🎯 Confidence Statistics:\")\n",
    "            print(f\"  Mean confidence: {results_df['confidence'].mean():.3f}\")\n",
    "            print(f\"  Median confidence: {results_df['confidence'].median():.3f}\")\n",
    "            high_conf = (results_df['confidence'] > 0.8).sum()\n",
    "            print(f\"  High confidence (>0.8): {high_conf:,} ({high_conf/len(results_df)*100:.1f}%)\")\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "# Main execution functions\n",
    "def run_bert_emotion_analysis(resume_from_checkpoint=True):\n",
    "    \"\"\"\n",
    "    Main function to run BERT emotion analysis\n",
    "    \n",
    "    Parameters:\n",
    "    - resume_from_checkpoint: Whether to resume from existing checkpoint (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🤖 BERT-based Goodreads Book Description Emotion Analysis\")\n",
    "    print(\"📚 Processing English books with descriptions only\")\n",
    "    print(\"🔄 Using fine-tuned BERT model from ArtEmis training\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Check configuration\n",
    "    if not os.path.exists(config.BERT_MODEL_DIR):\n",
    "        print(f\"❌ BERT model directory not found: {config.BERT_MODEL_DIR}\")\n",
    "        print(\"Please ensure you have:\")\n",
    "        print(\"1. Trained the model using utterance_to_emotion_with_transformer_my_try.py\")\n",
    "        print(\"2. Set do_training=True in that script\")\n",
    "        print(\"3. Updated the path to match your artemis directory structure\")\n",
    "        \n",
    "        # Suggest alternative paths\n",
    "        possible_paths = [\n",
    "            r'artemis\\predictions\\bert_based\\best_model',\n",
    "            r'data\\artemis\\artemis\\predictions\\bert_based\\best_model',\n",
    "            r'predictions\\bert_based\\best_model'\n",
    "        ]\n",
    "        print(\"\\nPossible model paths to check:\")\n",
    "        for path in possible_paths:\n",
    "            print(f\"  - {path}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    if not os.path.exists(config.DATA_FILE):\n",
    "        print(f\"❌ Data file not found: {config.DATA_FILE}\")\n",
    "        print(\"Please update DATA_FILE in the config above\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize and run predictor\n",
    "    predictor = BERTEmotionPredictor()\n",
    "    results_df = predictor.process_all_books(resume_from_checkpoint)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Convenience function for one-line execution\n",
    "def analyze_goodreads_bert_emotions(resume_from_checkpoint=True):\n",
    "    \"\"\"\n",
    "    One-line function to run the complete BERT emotion analysis\n",
    "    \n",
    "    Usage:\n",
    "    results = analyze_goodreads_bert_emotions()\n",
    "    \"\"\"\n",
    "    return run_bert_emotion_analysis(resume_from_checkpoint)\n",
    "\n",
    "# Display configuration\n",
    "print(\"📋 BERT EMOTION ANALYSIS CONFIGURATION\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"📁 Artemis path: {config.ARTEMIS_PATH}\")\n",
    "print(f\"🤖 BERT model directory: {config.BERT_MODEL_DIR}\")\n",
    "print(f\"📊 Data file: {config.DATA_FILE}\")\n",
    "print(f\"💾 Results directory: {config.RESULTS_DIR}\")\n",
    "print(f\"🖥️ Device: {config.DEVICE}\")\n",
    "print(f\"📦 Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"📏 Max sequence length: {config.MAX_LENGTH}\")\n",
    "print(f\"🎯 Expected emotion classes: {len(ARTEMIS_EMOTIONS) if 'ARTEMIS_EMOTIONS' in globals() else 'Not loaded'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check if model exists and show status\n",
    "if os.path.exists(config.BERT_MODEL_DIR):\n",
    "    model_files = os.listdir(config.BERT_MODEL_DIR)\n",
    "    print(f\"✅ Model directory found with files: {model_files}\")\n",
    "    print(\"Ready to run! Execute: results = analyze_goodreads_bert_emotions()\")\n",
    "else:\n",
    "    print(f\"❌ Model directory not found: {config.BERT_MODEL_DIR}\")\n",
    "    print(\"Please train the BERT model first using utterance_to_emotion_with_transformer_my_try.py\")\n",
    "\n",
    "results = analyze_goodreads_bert_emotions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c8fe0b-65cb-4a62-b259-0029d396f9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎨 Creating comprehensive BERT emotion analysis visualizations...\n",
      "============================================================\n",
      "🎭 BERT EMOTION ANALYSIS SUMMARY\n",
      "============================================================\n",
      "📚 Total books analyzed: 686,946\n",
      "🎯 Mean confidence: 0.481\n",
      "📊 Median confidence: 0.441\n",
      "📏 Average description length: 891 characters\n",
      "\n",
      "🎭 EMOTION DISTRIBUTION:\n",
      "  sadness        : 192,494 ( 28.0%)\n",
      "  amusement      : 110,417 ( 16.1%)\n",
      "  excitement     : 100,331 ( 14.6%)\n",
      "  fear           : 94,141 ( 13.7%)\n",
      "  awe            : 89,756 ( 13.1%)\n",
      "  anger          : 35,642 (  5.2%)\n",
      "  contentment    : 26,742 (  3.9%)\n",
      "  something else : 19,989 (  2.9%)\n",
      "  disgust        : 17,434 (  2.5%)\n",
      "\n",
      "📈 EMOTION CATEGORIES:\n",
      "                1: 339,711 ( 49.5%)\n",
      "                0: 327,246 ( 47.6%)\n",
      "                2: 19,989 (  2.9%)\n",
      "\n",
      "🎯 CONFIDENCE ANALYSIS:\n",
      "  Confidence ≥ 0.5: 265,905 ( 38.7%)\n",
      "  Confidence ≥ 0.6: 169,487 ( 24.7%)\n",
      "  Confidence ≥ 0.7: 102,818 ( 15.0%)\n",
      "  Confidence ≥ 0.8: 54,237 (  7.9%)\n",
      "  Confidence ≥ 0.9: 18,565 (  2.7%)\n",
      "\n",
      "🏆 MOST CONFIDENT PREDICTIONS:\n",
      "  amusement    (0.994): Zombie High Yearbook '64...\n",
      "  amusement    (0.994): Buck's Tooth...\n",
      "  amusement    (0.993): The Adventures of George...\n",
      "  amusement    (0.993): Exhaust(ed): The 99% True Story of a Bus Trip Gone...\n",
      "  amusement    (0.993): Cowpoke Clyde and Dirty Dawg...\n",
      "============================================================\n",
      "📊 Comprehensive visualization saved to goodreads_bert_emotion_results/visualizations/bert_emotion_analysis_comprehensive.png\n"
     ]
    }
   ],
   "source": [
    "# BERT Emotion Analysis Visualization Functions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_bert_visualizations(results_df, save_dir=None):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for BERT emotion analysis results\n",
    "    \n",
    "    Parameters:\n",
    "    - results_df: DataFrame with BERT emotion predictions\n",
    "    - save_dir: Directory to save plots (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    if results_df is None or len(results_df) == 0:\n",
    "        print(\"❌ No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 1. Emotion Distribution (Bar Plot)\n",
    "    plt.subplot(3, 3, 1)\n",
    "    emotion_counts = results_df['predicted_emotion'].value_counts()\n",
    "    bars = plt.bar(range(len(emotion_counts)), emotion_counts.values, alpha=0.8)\n",
    "    plt.xticks(range(len(emotion_counts)), emotion_counts.index, rotation=45, ha='right')\n",
    "    plt.title('Emotion Distribution in Goodreads Books\\n(BERT Predictions)', fontsize=12, pad=20)\n",
    "    plt.ylabel('Number of Books')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, value) in enumerate(zip(bars, emotion_counts.values)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, \n",
    "                f'{value:,}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 2. Emotion Categories Pie Chart\n",
    "    plt.subplot(3, 3, 2)\n",
    "    category_counts = results_df['emotion_category'].value_counts()\n",
    "    colors = ['#2ecc71', '#e74c3c', '#95a5a6']  # Green, Red, Gray\n",
    "    wedges, texts, autotexts = plt.pie(category_counts.values, labels=category_counts.index, \n",
    "                                      autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    plt.title('Emotion Categories Distribution\\n(Positive/Negative/Else)', fontsize=12, pad=20)\n",
    "    \n",
    "    # Enhance pie chart text\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    # 3. Confidence Distribution Histogram\n",
    "    plt.subplot(3, 3, 3)\n",
    "    plt.hist(results_df['confidence'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(results_df['confidence'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {results_df[\"confidence\"].mean():.3f}')\n",
    "    plt.axvline(results_df['confidence'].median(), color='orange', linestyle='--', \n",
    "                label=f'Median: {results_df[\"confidence\"].median():.3f}')\n",
    "    plt.title('Prediction Confidence Distribution', fontsize=12, pad=20)\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Number of Books')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Average Confidence by Emotion\n",
    "    plt.subplot(3, 3, 4)\n",
    "    emotion_confidence = results_df.groupby('predicted_emotion')['confidence'].agg(['mean', 'std']).sort_values('mean', ascending=False)\n",
    "    bars = plt.bar(range(len(emotion_confidence)), emotion_confidence['mean'].values, \n",
    "                   yerr=emotion_confidence['std'].values, alpha=0.8, capsize=5)\n",
    "    plt.xticks(range(len(emotion_confidence)), emotion_confidence.index, rotation=45, ha='right')\n",
    "    plt.title('Average Prediction Confidence by Emotion', fontsize=12, pad=20)\n",
    "    plt.ylabel('Average Confidence')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, value) in enumerate(zip(bars, emotion_confidence['mean'].values)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 5. Confidence vs Emotion Category\n",
    "    plt.subplot(3, 3, 5)\n",
    "    categories = results_df['emotion_category'].unique()\n",
    "    confidence_by_category = [results_df[results_df['emotion_category'] == cat]['confidence'].values \n",
    "                             for cat in categories]\n",
    "    \n",
    "    box_plot = plt.boxplot(confidence_by_category, labels=categories, patch_artist=True)\n",
    "    colors = ['lightgreen', 'lightcoral', 'lightgray']\n",
    "    for patch, color in zip(box_plot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    plt.title('Confidence Distribution by Emotion Category', fontsize=12, pad=20)\n",
    "    plt.ylabel('Confidence Score')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 6. Description Length vs Confidence\n",
    "    plt.subplot(3, 3, 6)\n",
    "    plt.scatter(results_df['description_length'], results_df['confidence'], alpha=0.5, s=10)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(results_df['description_length'], results_df['confidence'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(results_df['description_length'], p(results_df['description_length']), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.title('Description Length vs Prediction Confidence', fontsize=12, pad=20)\n",
    "    plt.xlabel('Description Length (characters)')\n",
    "    plt.ylabel('Confidence Score')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = results_df['description_length'].corr(results_df['confidence'])\n",
    "    plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    # 7. High Confidence Predictions by Emotion\n",
    "    plt.subplot(3, 3, 7)\n",
    "    high_conf_threshold = 0.8\n",
    "    high_conf_emotions = results_df[results_df['confidence'] > high_conf_threshold]['predicted_emotion'].value_counts()\n",
    "    \n",
    "    if len(high_conf_emotions) > 0:\n",
    "        bars = plt.bar(range(len(high_conf_emotions)), high_conf_emotions.values, alpha=0.8, color='gold')\n",
    "        plt.xticks(range(len(high_conf_emotions)), high_conf_emotions.index, rotation=45, ha='right')\n",
    "        plt.title(f'High Confidence Predictions (>{high_conf_threshold})', fontsize=12, pad=20)\n",
    "        plt.ylabel('Number of Books')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add percentages\n",
    "        total_high_conf = len(results_df[results_df['confidence'] > high_conf_threshold])\n",
    "        for i, (bar, value) in enumerate(zip(bars, high_conf_emotions.values)):\n",
    "            pct = value / total_high_conf * 100\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "                    f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No high confidence predictions', ha='center', va='center', \n",
    "                transform=plt.gca().transAxes, fontsize=12)\n",
    "        plt.title(f'High Confidence Predictions (>{high_conf_threshold})', fontsize=12, pad=20)\n",
    "    \n",
    "    # 8. Emotion Probability Heatmap (Top emotions)\n",
    "    plt.subplot(3, 3, 8)\n",
    "    \n",
    "    # Get top 6 most frequent emotions for readability\n",
    "    top_emotions = emotion_counts.head(6).index\n",
    "    prob_columns = [f'prob_{emotion.replace(\" \", \"_\")}' for emotion in top_emotions]\n",
    "    \n",
    "    # Sample data for heatmap (use first 100 books or less)\n",
    "    sample_size = min(100, len(results_df))\n",
    "    sample_df = results_df.head(sample_size)\n",
    "    prob_matrix = sample_df[prob_columns].values.T\n",
    "    \n",
    "    im = plt.imshow(prob_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    plt.colorbar(im, shrink=0.8)\n",
    "    plt.yticks(range(len(top_emotions)), top_emotions)\n",
    "    plt.xlabel('Book Samples')\n",
    "    plt.title(f'Emotion Probability Heatmap\\n(First {sample_size} books)', fontsize=12, pad=20)\n",
    "    \n",
    "    # 9. Summary Statistics Table\n",
    "    plt.subplot(3, 3, 9)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    stats = {\n",
    "        'Total Books Analyzed': f\"{len(results_df):,}\",\n",
    "        'Mean Confidence': f\"{results_df['confidence'].mean():.3f}\",\n",
    "        'Median Confidence': f\"{results_df['confidence'].median():.3f}\",\n",
    "        'High Confidence (>0.8)': f\"{(results_df['confidence'] > 0.8).sum():,} ({(results_df['confidence'] > 0.8).mean()*100:.1f}%)\",\n",
    "        'Most Common Emotion': f\"{emotion_counts.index[0]} ({emotion_counts.iloc[0]:,})\",\n",
    "        'Positive Emotions': f\"{(results_df['emotion_category'] == 'positive').sum():,} ({(results_df['emotion_category'] == 'positive').mean()*100:.1f}%)\",\n",
    "        'Negative Emotions': f\"{(results_df['emotion_category'] == 'negative').sum():,} ({(results_df['emotion_category'] == 'negative').mean()*100:.1f}%)\",\n",
    "        'Avg Description Length': f\"{results_df['description_length'].mean():.0f} chars\"\n",
    "    }\n",
    "    \n",
    "    # Create table\n",
    "    table_data = [[key, value] for key, value in stats.items()]\n",
    "    table = plt.table(cellText=table_data, colLabels=['Metric', 'Value'],\n",
    "                     cellLoc='left', loc='center', bbox=[0, 0, 1, 1])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 2)\n",
    "    \n",
    "    # Style the table\n",
    "    for i in range(len(stats) + 1):\n",
    "        table[(i, 0)].set_facecolor('#E8E8E8')\n",
    "        table[(i, 1)].set_facecolor('#F5F5F5')\n",
    "    \n",
    "    plt.title('Summary Statistics', fontsize=12, pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot if directory provided\n",
    "    if save_dir:\n",
    "        import os\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plt.savefig(f'{save_dir}/bert_emotion_analysis_comprehensive.png', \n",
    "                   dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"📊 Comprehensive visualization saved to {save_dir}/bert_emotion_analysis_comprehensive.png\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Additional detailed visualizations\n",
    "    create_detailed_emotion_analysis(results_df, save_dir)\n",
    "\n",
    "def create_detailed_emotion_analysis(results_df, save_dir=None):\n",
    "    \"\"\"Create additional detailed analysis plots\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Confidence distribution by emotion (violin plot)\n",
    "    ax1 = axes[0, 0]\n",
    "    emotions = results_df['predicted_emotion'].unique()\n",
    "    confidence_data = [results_df[results_df['predicted_emotion'] == emotion]['confidence'].values \n",
    "                      for emotion in emotions]\n",
    "    \n",
    "    parts = ax1.violinplot(confidence_data, positions=range(len(emotions)), showmeans=True)\n",
    "    ax1.set_xticks(range(len(emotions)))\n",
    "    ax1.set_xticklabels(emotions, rotation=45, ha='right')\n",
    "    ax1.set_title('Confidence Distribution by Emotion (Violin Plot)')\n",
    "    ax1.set_ylabel('Confidence Score')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Emotion transitions/correlations heatmap\n",
    "    ax2 = axes[0, 1]\n",
    "    from artemis.emotions import ARTEMIS_EMOTIONS\n",
    "    \n",
    "    # Create correlation matrix of emotion probabilities\n",
    "    prob_columns = [f'prob_{emotion.replace(\" \", \"_\")}' for emotion in ARTEMIS_EMOTIONS]\n",
    "    corr_matrix = results_df[prob_columns].corr()\n",
    "    \n",
    "    im = ax2.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    ax2.set_xticks(range(len(ARTEMIS_EMOTIONS)))\n",
    "    ax2.set_yticks(range(len(ARTEMIS_EMOTIONS)))\n",
    "    ax2.set_xticklabels(ARTEMIS_EMOTIONS, rotation=45, ha='right')\n",
    "    ax2.set_yticklabels(ARTEMIS_EMOTIONS)\n",
    "    ax2.set_title('Emotion Probability Correlations')\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(len(ARTEMIS_EMOTIONS)):\n",
    "        for j in range(len(ARTEMIS_EMOTIONS)):\n",
    "            text = ax2.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\" if abs(corr_matrix.iloc[i, j]) < 0.5 else \"white\")\n",
    "    \n",
    "    plt.colorbar(im, ax=ax2, shrink=0.8)\n",
    "    \n",
    "    # 3. Description length distribution by emotion category\n",
    "    ax3 = axes[1, 0]\n",
    "    categories = results_df['emotion_category'].unique()\n",
    "    length_data = [results_df[results_df['emotion_category'] == cat]['description_length'].values \n",
    "                  for cat in categories]\n",
    "    \n",
    "    box_plot = ax3.boxplot(length_data, labels=categories, patch_artist=True)\n",
    "    colors = ['lightgreen', 'lightcoral', 'lightgray']\n",
    "    for patch, color in zip(box_plot['boxes'], colors[:len(categories)]):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    ax3.set_title('Description Length by Emotion Category')\n",
    "    ax3.set_ylabel('Description Length (characters)')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Confidence threshold analysis\n",
    "    ax4 = axes[1, 1]\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    coverage = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        high_conf_mask = results_df['confidence'] >= threshold\n",
    "        coverage.append(high_conf_mask.mean() * 100)\n",
    "        \n",
    "        # For accuracy, we'd need ground truth labels\n",
    "        # For now, just show coverage\n",
    "    \n",
    "    ax4.plot(thresholds, coverage, 'b-', linewidth=2, label='Coverage %')\n",
    "    ax4.set_xlabel('Confidence Threshold')\n",
    "    ax4.set_ylabel('Coverage (%)')\n",
    "    ax4.set_title('Coverage vs Confidence Threshold')\n",
    "    ax4.grid(alpha=0.3)\n",
    "    ax4.legend()\n",
    "    \n",
    "    # Add some key points\n",
    "    for i, (thresh, cov) in enumerate(zip(thresholds[::5], coverage[::5])):\n",
    "        ax4.annotate(f'{cov:.1f}%', (thresh, cov), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        plt.savefig(f'{save_dir}/bert_detailed_analysis.png', \n",
    "                   dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"📊 Detailed analysis saved to {save_dir}/bert_detailed_analysis.png\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def print_emotion_summary(results_df):\n",
    "    \"\"\"Print a detailed text summary of the emotion analysis\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"🎭 BERT EMOTION ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"📚 Total books analyzed: {len(results_df):,}\")\n",
    "    print(f\"🎯 Mean confidence: {results_df['confidence'].mean():.3f}\")\n",
    "    print(f\"📊 Median confidence: {results_df['confidence'].median():.3f}\")\n",
    "    print(f\"📏 Average description length: {results_df['description_length'].mean():.0f} characters\")\n",
    "    \n",
    "    # Emotion distribution\n",
    "    print(f\"\\n🎭 EMOTION DISTRIBUTION:\")\n",
    "    emotion_counts = results_df['predicted_emotion'].value_counts()\n",
    "    for emotion, count in emotion_counts.items():\n",
    "        pct = count / len(results_df) * 100\n",
    "        print(f\"  {emotion:15}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Category distribution\n",
    "    print(f\"\\n📈 EMOTION CATEGORIES:\")\n",
    "    category_counts = results_df['emotion_category'].value_counts()\n",
    "    for category, count in category_counts.items():\n",
    "        pct = count / len(results_df) * 100\n",
    "        print(f\"  {category:15}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Confidence analysis\n",
    "    print(f\"\\n🎯 CONFIDENCE ANALYSIS:\")\n",
    "    conf_thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    for threshold in conf_thresholds:\n",
    "        high_conf = (results_df['confidence'] >= threshold).sum()\n",
    "        pct = high_conf / len(results_df) * 100\n",
    "        print(f\"  Confidence ≥ {threshold}: {high_conf:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Most confident predictions\n",
    "    print(f\"\\n🏆 MOST CONFIDENT PREDICTIONS:\")\n",
    "    top_confident = results_df.nlargest(5, 'confidence')[['title', 'predicted_emotion', 'confidence']]\n",
    "    for _, row in top_confident.iterrows():\n",
    "        print(f\"  {row['predicted_emotion']:12} ({row['confidence']:.3f}): {row['title'][:50]}...\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Usage function to run all visualizations\n",
    "def analyze_bert_results(results_df, save_dir=None):\n",
    "    \"\"\"\n",
    "    Run complete analysis and visualization of BERT results\n",
    "    \n",
    "    Usage after running BERT analysis:\n",
    "    analyze_bert_results(results, 'visualizations')\n",
    "    \"\"\"\n",
    "    \n",
    "    if save_dir is None:\n",
    "        save_dir = config.RESULTS_DIR if 'config' in globals() else 'bert_visualizations'\n",
    "    \n",
    "    print(\"🎨 Creating comprehensive BERT emotion analysis visualizations...\")\n",
    "    \n",
    "    # Print text summary\n",
    "    print_emotion_summary(results_df)\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_bert_visualizations(results_df, save_dir)\n",
    "    \n",
    "    print(f\"\\n✅ Analysis complete! Visualizations saved to: {save_dir}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example usage:\n",
    "# After running your BERT analysis:\n",
    "# results = analyze_goodreads_bert_emotions()\n",
    "\n",
    "vis_dir = f\"{config.RESULTS_DIR}/visualizations\"\n",
    "os.makedirs(vis_dir, exist_ok=True)\n",
    "analyze_bert_results(results, vis_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c21a42-7bae-448e-b500-93f2dcf9ec59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
