{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e25ea09-74d6-47e6-bad6-e922f52c3532",
   "metadata": {},
   "source": [
    "# Personal Information\n",
    "Name: **Friso Harlaar**\n",
    "\n",
    "StudentID: **12869384**\n",
    "\n",
    "Email: [**friso.harlaar@student.uva.nl**](friso.harlaar@student.uva.nl)\n",
    "\n",
    "Submitted on: **23.03.2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cf6243-adfe-4eb8-bba3-bb2835079abd",
   "metadata": {},
   "source": [
    "# Data Context\n",
    "**I will be using two main datasets in this thesis. The first one will contain images scraped manually from the [aesthetics wiki](https://aesthetics.fandom.com/wiki/Aesthetics_Wiki), it will be used to finetune a Visual Transformar to create an aesthetics classifier. The second dataset will be a books dataset, which contains metadata of books, such as the title, author(s), genre, etc. While also containing the description of the book, reviews and the cover image. This will be used to train a multimodal model which takes both the textual description, reviews, metadata and cover image as input and classify the book into an aesthetic.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833d964-56e1-49c7-8172-7435357624aa",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "**Present here the results of your exploratory data analysis. Note that there is no need to have a \"story line\" - it is more important that you show your understanding of the data and the methods that you will be using in your experiments (i.e. your methodology).**\n",
    "\n",
    "**As an example, you could show data, label, or group balances, skewness, and basic characterizations of the data. Information about data frequency and distributions as well as results from reduction mechanisms such as PCA could be useful. Furthermore, indicate outliers and how/why you are taking them out of your samples, if you do so.**\n",
    "\n",
    "**The idea is, that you conduct this analysis to a) understand the data better but b) also to verify the shapes of the distributions and whether they meet the assumptions of the methods that you will attempt to use. Finally, make good use of images, diagrams, and tables to showcase what information you have extracted from your data.**\n",
    "\n",
    "As you can see, you are in a jupyter notebook environment here. This means that you should focus little on writing text and more on actually exploring your data. If you need to, you can use the amsmath environment in-line: $e=mc^2$ or also in separate equations such as here:\n",
    "\n",
    "\\begin{equation}\n",
    "    e=mc^2 \\mathrm{\\space where \\space} e,m,c\\in \\mathbb{R}\n",
    "\\end{equation}\n",
    "\n",
    "Furthermore, you can insert images such as your data aggregation diagrams like this:\n",
    "\n",
    "![image](example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534317db-d881-4e33-a358-754e2881e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582b299-f599-4140-a454-bcbfdeeb273f",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7aa79",
   "metadata": {},
   "source": [
    "**Aesthetic images**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02f935-120e-4e62-ae4c-adc31975d1d7",
   "metadata": {},
   "source": [
    "These were scraped from the [aesthetics wiki](https://aesthetics.fandom.com/wiki/Aesthetics_Wiki). A list of 24 aesthetics, which was curated by [Giolo & Berghman](https://firstmonday.org/ojs/index.php/fm/article/view/12723), was used, however 2 of the 24 aesthetics were removed and the FrogCore aesthetic was made a subaesthetic, meaning that it doesn't have it's own page anymore, which made the scraping difficult. \n",
    "\n",
    "To create more training data, the images will be flipped horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0cf9be-2cac-4227-957f-ad893212e70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aesthetic</th>\n",
       "      <th>image_count</th>\n",
       "      <th>total_size_mb</th>\n",
       "      <th>avg_size_mb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frogcore</td>\n",
       "      <td>182</td>\n",
       "      <td>35.86</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kidcore</td>\n",
       "      <td>75</td>\n",
       "      <td>28.39</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Dark_Academia</td>\n",
       "      <td>63</td>\n",
       "      <td>17.28</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Fairy_Kei</td>\n",
       "      <td>60</td>\n",
       "      <td>7.77</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Traumacore</td>\n",
       "      <td>59</td>\n",
       "      <td>19.03</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cottagecore</td>\n",
       "      <td>55</td>\n",
       "      <td>21.11</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ethereal</td>\n",
       "      <td>50</td>\n",
       "      <td>12.76</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vaporwave</td>\n",
       "      <td>47</td>\n",
       "      <td>44.19</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bloomcore</td>\n",
       "      <td>40</td>\n",
       "      <td>11.29</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cyberpunk</td>\n",
       "      <td>33</td>\n",
       "      <td>28.30</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pastel_Goth</td>\n",
       "      <td>30</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Princesscore</td>\n",
       "      <td>29</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hellenic</td>\n",
       "      <td>28</td>\n",
       "      <td>28.37</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vibrant_Academia</td>\n",
       "      <td>22</td>\n",
       "      <td>7.97</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Goblincore</td>\n",
       "      <td>21</td>\n",
       "      <td>9.14</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Light_Academia</td>\n",
       "      <td>14</td>\n",
       "      <td>6.89</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Angura_Kei</td>\n",
       "      <td>14</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Virgo%27s_Tears</td>\n",
       "      <td>12</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Atompunk</td>\n",
       "      <td>11</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Monkeycore</td>\n",
       "      <td>10</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Anglo_Gothic</td>\n",
       "      <td>8</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Grandparentcore</td>\n",
       "      <td>7</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bubblegum_Witch</td>\n",
       "      <td>7</td>\n",
       "      <td>8.04</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           aesthetic  image_count  total_size_mb  avg_size_mb\n",
       "0           Frogcore          182          35.86         0.20\n",
       "14           Kidcore           75          28.39         0.38\n",
       "18     Dark_Academia           63          17.28         0.27\n",
       "16         Fairy_Kei           60           7.77         0.13\n",
       "19        Traumacore           59          19.03         0.32\n",
       "7        Cottagecore           55          21.11         0.38\n",
       "8           Ethereal           50          12.76         0.26\n",
       "6          Vaporwave           47          44.19         0.94\n",
       "10         Bloomcore           40          11.29         0.28\n",
       "3          Cyberpunk           33          28.30         0.86\n",
       "1        Pastel_Goth           30           3.21         0.11\n",
       "21      Princesscore           29           6.60         0.23\n",
       "15          Hellenic           28          28.37         1.01\n",
       "4   Vibrant_Academia           22           7.97         0.36\n",
       "17        Goblincore           21           9.14         0.44\n",
       "13    Light_Academia           14           6.89         0.49\n",
       "2         Angura_Kei           14           4.13         0.30\n",
       "12   Virgo%27s_Tears           12           2.46         0.21\n",
       "22          Atompunk           11           2.07         0.19\n",
       "9         Monkeycore           10           2.34         0.23\n",
       "20      Anglo_Gothic            8           0.27         0.03\n",
       "5    Grandparentcore            7           0.35         0.05\n",
       "11   Bubblegum_Witch            7           8.04         1.15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your data here\n",
    "base_path = \"data/aesthetic_images/\"\n",
    "\n",
    "# Get list of all aesthetic folders\n",
    "aesthetic_folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "\n",
    "# Create a list to store the counts\n",
    "counts = []\n",
    "\n",
    "# Count files in each folder and get additional statistics\n",
    "for aesthetic in aesthetic_folders:\n",
    "    folder_path = os.path.join(base_path, aesthetic)\n",
    "    image_files = glob.glob(os.path.join(folder_path, \"*\"))\n",
    "    \n",
    "    # Calculate total size in MB\n",
    "    total_size_bytes = sum(os.path.getsize(file) for file in image_files)\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "    \n",
    "    counts.append({\n",
    "        \"aesthetic\": aesthetic,\n",
    "        \"image_count\": len(image_files),\n",
    "        \"total_size_mb\": round(total_size_mb, 2),\n",
    "        \"avg_size_mb\": round(total_size_mb / len(image_files), 2) if image_files else 0\n",
    "    })\n",
    "\n",
    "# Sort by image count\n",
    "df_image_counts = pd.DataFrame(counts)\n",
    "df_image_counts = df_image_counts.sort_values(\"image_count\", ascending=False)\n",
    "df_image_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266e17c",
   "metadata": {},
   "source": [
    "**Books dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee6ee022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/goodreads/goodreads_books/goodreads_book_works.json.gz', 'data/goodreads/goodreads_books/goodreads_book_genres_initial.json.gz', 'data/goodreads/goodreads_books/goodreads_book_authors.json.gz', 'data/goodreads/goodreads_books/goodreads_book_series.json.gz', 'data/goodreads/goodreads_books/goodreads_books.json.gz']\n"
     ]
    }
   ],
   "source": [
    "# There are multiple files in the goodreads dataset\n",
    "# Here is an overview of each file:\n",
    "# https://cseweb.ucsd.edu/~jmcauley/datasets/goodreads.html\n",
    "BOOKS_PATH = r'data/goodreads/goodreads_books/'\n",
    "\n",
    "# All book datasets\n",
    "book_files = glob.glob(os.path.join(BOOKS_PATH, \"*.gz\"))\n",
    "\n",
    "print(book_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df9546-a6d7-4678-aca6-cd13d5f3c79a",
   "metadata": {},
   "source": [
    "### Analysis 1: \n",
    "Make sure to add some explanation of what you are doing in your code. This will help you and whoever will read this a lot in following your steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a889a6c7-aed8-4a0f-9925-c4f8e2fce1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f586a3b32e442eb80ebe59fda10ab59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100,000 records\n",
      "Processed 200,000 records\n",
      "Processed 300,000 records\n",
      "Processed 400,000 records\n",
      "Processed 500,000 records\n",
      "Processed 600,000 records\n",
      "Processed 700,000 records\n",
      "Processed 800,000 records\n",
      "Processed 900,000 records\n",
      "Processed 1,000,000 records\n",
      "Processed 1,100,000 records\n",
      "Processed 1,200,000 records\n",
      "Processed 1,300,000 records\n",
      "Processed 1,400,000 records\n",
      "Processed 1,500,000 records\n",
      "Processed 1,600,000 records\n",
      "Processed 1,700,000 records\n",
      "Processed 1,800,000 records\n",
      "Processed 1,900,000 records\n",
      "Processed 2,000,000 records\n",
      "Processed 2,100,000 records\n",
      "Processed 2,200,000 records\n",
      "Processed 2,300,000 records\n",
      "Creating DataFrame with 2,360,655 records...\n"
     ]
    }
   ],
   "source": [
    "MAIN_BOOKS_PATH = r'data/goodreads/goodreads_books/goodreads_books.json.gz'\n",
    "\n",
    "def read_goodreads_data(file_path, max_rows=None, sample_size=10000, return_sample=True):\n",
    "    \"\"\"\n",
    "    Read Goodreads JSON.GZ data into a DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the goodreads_books.json.gz file\n",
    "    max_rows : int, optional\n",
    "        Maximum number of rows to read (None = read all)\n",
    "    sample_size : int, optional\n",
    "        Number of rows to sample if return_sample=True\n",
    "    return_sample : bool, default=True\n",
    "        If True, return a random sample instead of the full dataset\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame containing book data\n",
    "    \"\"\"\n",
    "    all_books = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    # For sampling\n",
    "    if return_sample:\n",
    "        # First pass to count total lines (if we need exact sampling)\n",
    "        if not max_rows:\n",
    "            print(\"Counting total records for sampling...\")\n",
    "            with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                total_lines = sum(1 for _ in tqdm(f))\n",
    "            sampling_rate = min(1.0, sample_size / total_lines)\n",
    "            print(f\"Sampling rate: {sampling_rate:.4f} ({sample_size} of {total_lines:,})\")\n",
    "        else:\n",
    "            # If max_rows is specified, use that for sampling rate calculation\n",
    "            total_lines = max_rows\n",
    "            sampling_rate = min(1.0, sample_size / max_rows)\n",
    "    \n",
    "    # Read the file\n",
    "    print(f\"Reading data{' (sampling)' if return_sample else ''}...\")\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in tqdm(enumerate(f)):\n",
    "            # Stop if we reached max_rows\n",
    "            if max_rows and i >= max_rows:\n",
    "                break\n",
    "                \n",
    "            # Sample if requested\n",
    "            if return_sample and np.random.random() > sampling_rate:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Parse JSON line and append to list\n",
    "                book = json.loads(line.strip())\n",
    "                all_books.append(book)\n",
    "                total_processed += 1\n",
    "                \n",
    "                # Print progress for large datasets\n",
    "                if total_processed % 100000 == 0 and not return_sample:\n",
    "                    print(f\"Processed {total_processed:,} records\")\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error parsing JSON at line {i}\")\n",
    "    \n",
    "    print(f\"Creating DataFrame with {len(all_books):,} records...\")\n",
    "    df = pd.DataFrame(all_books)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 1. Get a sample of books (fastest)\n",
    "# sample_df = read_goodreads_data(\n",
    "#     MAIN_BOOKS_PATH, \n",
    "#     return_sample=True, \n",
    "#     sample_size=10000\n",
    "# )\n",
    "# print(f\"Sample DataFrame shape: {sample_df.shape}\")\n",
    "# sample_df.head()\n",
    "\n",
    "# 2. Read the first N books\n",
    "# first_n_df = read_goodreads_data(\n",
    "#    '../goodreads/goodreads_books/goodreads_books.json.gz',\n",
    "#    max_rows=100000,\n",
    "#    return_sample=False\n",
    "# )\n",
    "\n",
    "# 3. Read all books (requires a lot of memory)\n",
    "df = read_goodreads_data(\n",
    "   MAIN_BOOKS_PATH,\n",
    "   return_sample=False\n",
    ")\n",
    "df.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2fdab97-d7c4-48d7-86c8-f054af4db287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['isbn', 'text_reviews_count', 'series', 'country_code', 'language_code',\n",
       "       'popular_shelves', 'asin', 'is_ebook', 'average_rating', 'kindle_asin',\n",
       "       'similar_books', 'description', 'format', 'link', 'authors',\n",
       "       'publisher', 'num_pages', 'publication_day', 'isbn13',\n",
       "       'publication_month', 'edition_information', 'publication_year', 'url',\n",
       "       'image_url', 'book_id', 'ratings_count', 'work_id', 'title',\n",
       "       'title_without_series'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(981061, 29)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "isbn                     983373\n",
       "text_reviews_count          524\n",
       "series                        0\n",
       "country_code                490\n",
       "language_code           1060153\n",
       "popular_shelves               0\n",
       "asin                    1891138\n",
       "is_ebook                    490\n",
       "average_rating              524\n",
       "kindle_asin             1345725\n",
       "similar_books                 0\n",
       "description              412233\n",
       "format                   646754\n",
       "link                        524\n",
       "authors                       0\n",
       "publisher                654362\n",
       "num_pages                764133\n",
       "publication_day         1024429\n",
       "isbn13                   780263\n",
       "publication_month        882945\n",
       "edition_information     2142642\n",
       "publication_year         599625\n",
       "url                         524\n",
       "image_url                   490\n",
       "book_id                       0\n",
       "ratings_count               524\n",
       "work_id                     524\n",
       "title                         7\n",
       "title_without_series          7\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.columns)\n",
    "NO_IMAGE_LINK = 'https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png'\n",
    "display(df[df['image_url'] == NO_IMAGE_LINK].shape)\n",
    "display(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fbbc5c0-d084-4402-bbc9-fa1f41005a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285692, 29)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df[(df['image_url'] == NO_IMAGE_LINK) & (df['description'].isna())].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b796dc-f69d-4686-b802-bd0d8f679ee8",
   "metadata": {},
   "source": [
    "### Analysis 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b453f-1bc2-4cad-8021-e548d307f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273bea3-ecaa-4fac-83d0-5fe547b7873d",
   "metadata": {},
   "source": [
    "### Analysis n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60074a1b-1ae5-46e8-971f-100199861c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
